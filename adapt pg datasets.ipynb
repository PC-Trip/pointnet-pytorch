{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch_geometric.datasets import ModelNet\n",
    "import torch_geometric.transforms as T\n",
    "\n",
    "import mlflow\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch_geometric.nn import DynamicEdgeConv, global_max_pool\n",
    "\n",
    "from pointnet import PointNetCls\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pptk\n",
    "\n",
    "def view(points):\n",
    "    v = pptk.viewer(points)\n",
    "    v.attributes(points)\n",
    "    v.set(point_size=0.01)\n",
    "    # selected = points[v.get('selected')]\n",
    "    return v\n",
    "\n",
    "def plot_conf_matrix(conf_mtrx, labels, file_path='temp_conf_mtrx.png'):\n",
    "    df_cm = pd.DataFrame(np.array(conf_mtrx), index=labels, columns=labels)\n",
    "    plt.figure(figsize = (10,7))\n",
    "    sn.heatmap(df_cm, annot=True)\n",
    "    plt.savefig(file_path)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'ModelNet' object has no attribute 'dataset'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-e3668d41cc82>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[0mtrain_loader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mAdapter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatchsize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_workers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[0mtest_loader\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mAdapter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatchsize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_workers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m \u001b[0mnum2cat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraw_file_names\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'ModelNet' object has no attribute 'dataset'"
     ]
    }
   ],
   "source": [
    "batchsize = 32\n",
    "blue = lambda x: '\\033[94m' + x + '\\033[0m'\n",
    "yellow = lambda x: '\\033[93m' + x + '\\033[0m'\n",
    "red = lambda x: '\\033[91m' + x + '\\033[0m'\n",
    "pre_transform, transform = T.NormalizeScale(), T.SamplePoints(1024)\n",
    "train_dataset = ModelNet('../data/modelnet10', '10', True, transform, pre_transform)\n",
    "test_dataset = ModelNet('../data/modelnet10', '10', False, transform, pre_transform)\n",
    "\n",
    "class Adapter:\n",
    "    def __init__(self, pg_dataset):\n",
    "        self.pg_dataset = pg_dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pg_dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.pg_dataset[idx].pos.numpy(), self.pg_dataset[idx].y.numpy().item() \n",
    "    \n",
    "train_loader = torch.utils.data.DataLoader(Adapter(train_dataset), batch_size=batchsize, shuffle=True, num_workers=0)\n",
    "test_loader  = torch.utils.data.DataLoader(Adapter(test_dataset), batch_size=batchsize, shuffle=False, num_workers=0)\n",
    "num2cat = dict(zip(range(10), train_dataset.raw_file_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1: 1/125 | train loss: 2.454874 | train acc: 0.031250\n",
      "epoch 1: 2/125 | train loss: 2.453289 | train acc: 0.093750\n",
      "epoch 1: 3/125 | train loss: 1.969043 | train acc: 0.250000\n",
      "epoch 1: 4/125 | train loss: 2.060935 | train acc: 0.250000\n",
      "epoch 1: 5/125 | train loss: 1.517412 | train acc: 0.562500\n",
      "epoch 1: 6/125 | train loss: 1.800226 | train acc: 0.500000\n",
      "epoch 1: 7/125 | train loss: 2.053360 | train acc: 0.375000\n",
      "epoch 1: 8/125 | train loss: 1.998027 | train acc: 0.343750\n",
      "epoch 1: 9/125 | train loss: 1.827359 | train acc: 0.375000\n",
      "epoch 1: 10/125 | train loss: 1.960671 | train acc: 0.468750\n",
      "epoch 1: 10/125 | test loss: 18.070076 | test acc: 0.000000\n",
      "epoch 1: 11/125 | train loss: 1.342815 | train acc: 0.500000\n",
      "epoch 1: 12/125 | train loss: 1.126386 | train acc: 0.531250\n",
      "epoch 1: 13/125 | train loss: 1.429792 | train acc: 0.375000\n",
      "epoch 1: 14/125 | train loss: 1.612843 | train acc: 0.562500\n",
      "epoch 1: 15/125 | train loss: 1.285018 | train acc: 0.593750\n",
      "epoch 1: 16/125 | train loss: 1.375944 | train acc: 0.593750\n",
      "epoch 1: 17/125 | train loss: 1.454069 | train acc: 0.437500\n",
      "epoch 1: 18/125 | train loss: 1.380967 | train acc: 0.500000\n",
      "epoch 1: 19/125 | train loss: 1.262533 | train acc: 0.656250\n",
      "epoch 1: 20/125 | train loss: 1.113906 | train acc: 0.656250\n",
      "epoch 1: 20/125 | test loss: 5.230262 | test acc: 0.000000\n",
      "epoch 1: 21/125 | train loss: 1.415496 | train acc: 0.500000\n",
      "epoch 1: 22/125 | train loss: 1.350814 | train acc: 0.468750\n",
      "epoch 1: 23/125 | train loss: 1.007922 | train acc: 0.687500\n",
      "epoch 1: 24/125 | train loss: 1.349117 | train acc: 0.468750\n",
      "epoch 1: 25/125 | train loss: 0.808367 | train acc: 0.750000\n",
      "epoch 1: 26/125 | train loss: 1.299294 | train acc: 0.500000\n",
      "epoch 1: 27/125 | train loss: 1.079641 | train acc: 0.562500\n",
      "epoch 1: 28/125 | train loss: 1.237356 | train acc: 0.656250\n",
      "epoch 1: 29/125 | train loss: 1.674159 | train acc: 0.406250\n",
      "epoch 1: 30/125 | train loss: 1.287963 | train acc: 0.468750\n",
      "epoch 1: 30/125 | test loss: 4.252997 | test acc: 0.000000\n",
      "epoch 1: 31/125 | train loss: 1.143349 | train acc: 0.500000\n",
      "epoch 1: 32/125 | train loss: 1.268671 | train acc: 0.531250\n",
      "epoch 1: 33/125 | train loss: 1.327365 | train acc: 0.562500\n",
      "epoch 1: 34/125 | train loss: 1.113153 | train acc: 0.625000\n",
      "epoch 1: 35/125 | train loss: 1.225489 | train acc: 0.500000\n",
      "epoch 1: 36/125 | train loss: 1.262464 | train acc: 0.531250\n"
     ]
    }
   ],
   "source": [
    "num_classes = len(num2cat)\n",
    "num_batch = len(train_dataset)/batchsize\n",
    "\n",
    "classifier = PointNetCls(k=num_classes)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "classifier.to(device)\n",
    "optimizer = optim.Adam(classifier.parameters(), lr=0.01)    \n",
    "scheduler = StepLR(optimizer, step_size=1.0, gamma=0.99)\n",
    "mlflow.log_param('Start', 'From zero')\n",
    "\n",
    "step = 0\n",
    "for epoch in range(1):\n",
    "    train_acc_epoch, test_acc_epoch = [], []\n",
    "    for i, data in enumerate(train_loader):\n",
    "        points, labels = data\n",
    "        points = points.transpose(2, 1)\n",
    "        points, labels = points.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        classifier = classifier.train()\n",
    "        pred, _ = classifier(points)\n",
    "        pred = pred.view(-1, num_classes)\n",
    "        loss = F.nll_loss(pred, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        pred_choice = pred.data.max(1)[1]\n",
    "        correct = pred_choice.eq(labels.data).cpu().sum()\n",
    "        train_acc = correct.item() / float(batchsize)\n",
    "        print('epoch %d: %d/%d | train loss: %f | train acc: %f' % (epoch+1, i+1, num_batch+1, loss.item(), train_acc))\n",
    "        train_acc_epoch.append(train_acc)\n",
    "        \n",
    "        # log train\n",
    "        encountered_class_nums = np.unique( np.array(labels.cpu().tolist()))\n",
    "        prec, recall, f1, _ = precision_recall_fscore_support(labels.cpu().tolist(), pred_choice.cpu().tolist(), labels=sorted(encountered_class_nums))\n",
    "        for j, num in enumerate(encountered_class_nums):\n",
    "            name = num2cat[num]\n",
    "            mlflow.log_metrics({\n",
    "                f\"{name}_acc\": prec[j],\n",
    "                f\"{name}_recall\": recall[j],\n",
    "                f\"{name}_f1\": f1[j],\n",
    "            }, step=step)\n",
    "        cnf_mtrx = confusion_matrix(labels.cpu().tolist(), pred_choice.cpu().tolist(), labels=sorted(list(num2cat)))\n",
    "        conf_mtrx_file_path = os.path.join(\"temp\", f\"train_cnf_mtrx_{epoch}_{i}.png\")\n",
    "        plot_conf_matrix(cnf_mtrx, [num2cat[num] for num in sorted(list(num2cat))], conf_mtrx_file_path)\n",
    "        mlflow.log_artifact(conf_mtrx_file_path)\n",
    "        mlflow.log_metric('train_acc', train_acc, step=step)\n",
    "\n",
    "        if (i+1) % 10 == 0:\n",
    "            j, data = next(enumerate(test_loader, 0))\n",
    "            points, labels = data\n",
    "            points = points.transpose(2, 1)\n",
    "            points, labels = points.to(device), labels.to(device)\n",
    "            classifier = classifier.eval()\n",
    "            with torch.no_grad():\n",
    "                pred, _ = classifier(points)\n",
    "            pred = pred.view(-1, num_classes)\n",
    "            loss = F.nll_loss(pred, labels)\n",
    "            pred_choice = pred.data.max(1)[1]\n",
    "            correct = pred_choice.eq(labels.data).cpu().sum()\n",
    "            test_acc = correct.item() / float(batchsize)\n",
    "            print(blue('epoch %d: %d/%d | test loss: %f | test acc: %f') % (epoch+1, i+1, num_batch+1, loss.item(), test_acc))\n",
    "            test_acc_epoch.append(test_acc)\n",
    "            \n",
    "            # log test\n",
    "            cnf_mtrx = confusion_matrix(labels.cpu().tolist(), pred_choice.cpu().tolist(), labels=sorted(list(num2cat)))\n",
    "            conf_mtrx_file_path = os.path.join(\"temp\", f\"test_cnf_mtrx_{epoch}_{i}.png\")\n",
    "            plot_conf_matrix(cnf_mtrx, [num2cat[num] for num in sorted(list(num2cat))], conf_mtrx_file_path)\n",
    "            mlflow.log_artifact(conf_mtrx_file_path)\n",
    "            mlflow.log_metric('test_acc', np.mean(test_acc_epoch))\n",
    "            \n",
    "        # to next batch\n",
    "        step += 1\n",
    "    \n",
    "    # to next epoch\n",
    "    print(yellow('epoch %d | mean train acc: %f') % (epoch+1, np.mean(train_acc_epoch)))\n",
    "    print(red('epoch %d | mean test acc: %f') % (epoch+1, np.mean(test_acc_epoch)))\n",
    "    torch.save(classifier.state_dict(), '%s/%s_model_%d.pth' % ('models', 'model_10', epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "num2class = dict(zip(range(10), train_loader.dataset.raw_file_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bed\n",
      "-0.8752633 0.89645064 1.771714\n",
      "-0.99999905 0.89762324 1.8976223\n",
      "-0.231328 0.4611699 0.6924979\n",
      "\n",
      "desk\n",
      "-0.25317165 0.8512118 1.1043835\n",
      "-0.95747113 0.999999 1.9574702\n",
      "-0.6959119 0.12586308 0.82177496\n",
      "\n",
      "sofa\n",
      "-0.71917844 0.6585192 1.3776977\n",
      "-0.8147365 0.99999905 1.8147355\n",
      "-0.1762224 0.4483075 0.6245299\n",
      "\n",
      "chair\n",
      "-0.6121519 0.58003956 1.1921915\n",
      "-0.8185277 0.99923277 1.8177605\n",
      "-0.56900716 0.8437649 1.412772\n",
      "\n",
      "table\n",
      "-0.2510807 0.5558331 0.8069138\n",
      "-0.36220503 0.36191383 0.7241188\n",
      "-0.999999 0.10629373 1.1062927\n",
      "\n",
      "night_stand\n",
      "-0.10616867 0.7600212 0.8661899\n",
      "-0.79955983 0.79955995 1.5991198\n",
      "-0.999999 0.9989007 1.9988997\n",
      "\n",
      "monitor\n",
      "-0.3327983 0.999999 1.3327973\n",
      "-0.66639864 0.66639864 1.3327973\n",
      "-0.5120062 0.8207911 1.3327973\n",
      "\n",
      "monitor\n",
      "-0.1636657 0.39862558 0.56229126\n",
      "-0.6964982 0.67877895 1.3752772\n",
      "-0.28227845 0.999999 1.2822775\n",
      "\n",
      "sofa\n",
      "-0.92549175 0.36221954 1.2877113\n",
      "-0.46057156 0.9548646 1.4154361\n",
      "-0.2584274 0.10073888 0.3591663\n",
      "\n",
      "sofa\n",
      "-0.5206773 0.2951557 0.81583303\n",
      "-0.99939686 0.99364084 1.9930377\n",
      "-0.48419538 0.30367824 0.7878736\n",
      "\n",
      "toilet\n",
      "-0.4552204 0.50236785 0.95758826\n",
      "-0.76645243 0.6034657 1.3699181\n",
      "-0.71473897 0.95712274 1.6718616\n",
      "\n",
      "dresser\n",
      "-0.114071645 0.62443316 0.7385048\n",
      "-0.61424726 0.6165246 1.2307718\n",
      "-0.8530104 0.999999 1.8530095\n",
      "\n",
      "table\n",
      "-0.6117345 0.63331324 1.2450478\n",
      "-0.99751294 0.99999905 1.997512\n",
      "-0.3702162 0.15196715 0.52218336\n",
      "\n",
      "monitor\n",
      "-0.26349068 0.67495644 0.9384471\n",
      "-0.72137904 0.99999905 1.7213781\n",
      "-0.40282714 0.82389855 1.2267257\n",
      "\n",
      "sofa\n",
      "-0.29866958 0.41310963 0.71177924\n",
      "-0.99999905 0.99783474 1.9978337\n",
      "-0.24019453 0.32201758 0.5622121\n",
      "\n",
      "bathtub\n",
      "-0.8789597 0.7400989 1.6190586\n",
      "-0.99999905 0.61905956 1.6190586\n",
      "-0.16641697 0.33953804 0.505955\n",
      "\n",
      "table\n",
      "-0.47320879 0.48140267 0.9546114\n",
      "-0.99999905 0.97148114 1.9714801\n",
      "-0.43225828 0.22144301 0.6537013\n",
      "\n",
      "chair\n",
      "-0.38480023 0.39509654 0.77989674\n",
      "-0.3967622 0.5232639 0.92002606\n",
      "-0.7747185 0.999999 1.7747176\n",
      "\n",
      "monitor\n",
      "-0.015631767 0.286343 0.30197477\n",
      "-0.53088284 0.8591431 1.3900259\n",
      "-0.24118198 0.999999 1.241181\n",
      "\n",
      "toilet\n",
      "-0.4235513 0.5099513 0.93350255\n",
      "-0.9911959 0.5632827 1.5544786\n",
      "-0.8378616 0.76256543 1.600427\n",
      "\n",
      "monitor\n",
      "-0.08780523 0.3969442 0.48474944\n",
      "-0.94189286 0.9346774 1.8765702\n",
      "-0.62281144 0.9999989 1.6228104\n",
      "\n",
      "monitor\n",
      "-0.3458625 0.6002558 0.9461183\n",
      "-0.6383226 0.9166274 1.55495\n",
      "-0.45766035 0.9999191 1.4575795\n",
      "\n",
      "bed\n",
      "-0.4807979 0.48151955 0.96231747\n",
      "-0.999999 0.32732812 1.3273271\n",
      "-0.29757348 0.22321852 0.520792\n",
      "\n",
      "sofa\n",
      "-0.9999989 0.48033732 1.4803362\n",
      "-0.8890247 0.82903904 1.7180637\n",
      "-0.17606185 0.15890658 0.33496845\n",
      "\n",
      "bed\n",
      "-0.26995608 0.46525472 0.7352108\n",
      "-0.999999 0.5865086 1.5865076\n",
      "-0.3628139 0.3288647 0.6916786\n",
      "\n",
      "chair\n",
      "-0.43191916 0.37656483 0.80848396\n",
      "-0.44643652 0.37111592 0.81755245\n",
      "-0.4653814 0.9946074 1.4599888\n",
      "\n",
      "sofa\n",
      "-0.5015735 0.4716685 0.97324204\n",
      "-0.9990344 0.9894393 1.9884737\n",
      "-0.30356026 0.3957905 0.6993507\n",
      "\n",
      "toilet\n",
      "-0.61118966 0.8925782 1.5037678\n",
      "-0.9995026 0.7635262 1.7630289\n",
      "-0.75148773 0.7567594 1.5082471\n",
      "\n",
      "dresser\n",
      "-0.19156232 0.92825 1.1198124\n",
      "-0.9999989 0.86635476 1.8663538\n",
      "-0.67663735 0.5675985 1.2442359\n",
      "\n",
      "dresser\n",
      "-0.20152065 0.61138785 0.81290853\n",
      "-0.61225384 0.61225384 1.2245077\n",
      "-0.99999905 0.5640781 1.5640771\n",
      "\n",
      "bed\n",
      "-0.6828997 0.6906824 1.3735821\n",
      "-0.994555 0.99948746 1.9940424\n",
      "-0.43202317 0.5505934 0.98261654\n",
      "\n",
      "sofa\n",
      "-0.31156394 0.40073374 0.7122977\n",
      "-0.99947596 0.999999 1.999475\n",
      "-0.2959003 0.25023946 0.5461397\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pointss = data.pos.numpy()\n",
    "for btch in np.unique(data.batch.numpy()):\n",
    "    points = pointss[data.batch == btch]\n",
    "    print(num2class[data.y[btch].numpy().item()])\n",
    "    print(np.min(points[:, 0], axis=0), np.max(points[:, 0], axis=0), np.max(points[:, 0], axis=0) - np.min(points[:, 0], axis=0))\n",
    "    print(np.min(points[:, 1], axis=0), np.max(points[:, 1], axis=0), np.max(points[:, 1], axis=0) - np.min(points[:, 1], axis=0))\n",
    "    print(np.min(points[:, 2], axis=0), np.max(points[:, 2], axis=0), np.max(points[:, 2], axis=0) - np.min(points[:, 2], axis=0))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 7 1 7 7 8 1 7 2 1 6 7 2 4 9 2 5 9 6 8 7 2 8 9 1 5 9 3 2 7 5 7]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pptk.viewer.viewer.viewer at 0x15ea8f32588>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for data in train_loader:\n",
    "    pass\n",
    "    break\n",
    "    \n",
    "print(data.y.numpy())\n",
    "points = data.pos.numpy()[data.batch == 0]\n",
    "view(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
