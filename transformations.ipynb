{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch_geometric.datasets import ModelNet\n",
    "import torch_geometric.transforms as T\n",
    "\n",
    "from random import uniform\n",
    "from tqdm import tqdm\n",
    "import mlflow\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch_geometric.nn import DynamicEdgeConv, global_max_pool\n",
    "\n",
    "from pointnet import PointNetCls\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_fscore_support, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pptk\n",
    "\n",
    "def view(points):\n",
    "    v = pptk.viewer(points)\n",
    "    v.attributes(points)\n",
    "    v.set(point_size=0.01)\n",
    "    # selected = points[v.get('selected')]\n",
    "    return v\n",
    "\n",
    "def plot_conf_matrix(conf_mtrx, labels, file_path='temp_conf_mtrx.png'):\n",
    "    df_cm = pd.DataFrame(np.array(conf_mtrx), index=labels, columns=labels)\n",
    "    plt.figure(figsize = (10,7))\n",
    "    sn.heatmap(df_cm, annot=True)\n",
    "    plt.savefig(file_path)\n",
    "    plt.close()\n",
    "    \n",
    "def get_path_of_last_model():\n",
    "    models_path = 'models'\n",
    "    files = list(filter(lambda f: os.path.isfile(os.path.join(models_path, f)) and f.endswith('.pth'), os.listdir(models_path) ))\n",
    "    if len(files) == 0:\n",
    "        return None, 0\n",
    "    files.sort(key=lambda f: int(f.split('.')[0].split('_')[-1] ))\n",
    "    return os.path.join(models_path, files[-1]), int(files[-1].split('.')[0].split('_')[-1])\n",
    "\n",
    "def test_model_full(classifier, test_data, num2cat, step=0, model_epoch_cumulatiove_base=0):\n",
    "    all_labels = []\n",
    "    all_choice = []\n",
    "    for j, data in enumerate(test_loader, 0):\n",
    "        points, labels = data\n",
    "        points = points.transpose(2, 1)\n",
    "        points, labels = points.to(device), labels.to(device)\n",
    "        classifier = classifier.eval()\n",
    "        with torch.no_grad():\n",
    "            pred, _ = classifier(points)\n",
    "        pred = pred.view(-1, num_classes)\n",
    "        loss = F.nll_loss(pred, labels)\n",
    "        pred_choice = pred.data.max(1)[1]\n",
    "            \n",
    "        all_labels.append(labels.cpu().numpy())\n",
    "        all_choice.append(pred_choice.cpu().numpy())\n",
    "            \n",
    "    all_labels = np.concatenate(all_labels)\n",
    "    all_choice = np.concatenate(all_choice)\n",
    "    test_acc = accuracy_score(all_labels, all_choice)\n",
    "    print(blue('epoch %d: %d/%d | test loss: %f | test acc: %f') % (model_epoch_cumulatiove_base+epoch+1, i+1, num_batch+1, loss.item(), test_acc))\n",
    "\n",
    "    cnf_mtrx = confusion_matrix(all_labels, all_choice, labels=sorted(list(num2cat)))\n",
    "    conf_mtrx_file_path = os.path.join(\"temp\", f\"test_cnf_mtrx_{epoch}_{i}.png\")\n",
    "    plot_conf_matrix(cnf_mtrx, [num2cat[num] for num in sorted(list(num2cat))], conf_mtrx_file_path)\n",
    "    mlflow.log_artifact(conf_mtrx_file_path)\n",
    "    mlflow.log_metric('test_acc', test_acc, step=step)\n",
    "    return test_acc\n",
    "    \n",
    "def test_model_simple(model, test_loader, step=0):\n",
    "    j, data = next(enumerate(test_loader, 0))\n",
    "    points, labels = data\n",
    "    points = points.transpose(2, 1)\n",
    "    points, labels = points.to(device), labels.to(device)\n",
    "    classifier = classifier.eval()\n",
    "    with torch.no_grad():\n",
    "        pred, _ = classifier(points)\n",
    "    pred = pred.view(-1, num_classes)\n",
    "    loss = F.nll_loss(pred, labels)\n",
    "    pred_choice = pred.data.max(1)[1]\n",
    "    correct = pred_choice.eq(labels.data).cpu().sum()\n",
    "    test_acc = correct.item() / float(batchsize)\n",
    "    print(blue('epoch %d: %d/%d | test loss: %f | test acc: %f') % (model_epoch_cumulatiove_base+epoch+1, i+1, num_batch+1, loss.item(), test_acc))\n",
    "\n",
    "    # log test\n",
    "    cnf_mtrx = confusion_matrix(labels.cpu().tolist(), pred_choice.cpu().tolist(), labels=sorted(list(num2cat)))\n",
    "    conf_mtrx_file_path = os.path.join(\"temp\", f\"test_cnf_mtrx_{epoch}_{i}.png\")\n",
    "    plot_conf_matrix(cnf_mtrx, [num2cat[num] for num in sorted(list(num2cat))], conf_mtrx_file_path)\n",
    "    mlflow.log_artifact(conf_mtrx_file_path)\n",
    "    mlflow.log_metric('test_acc', np.mean(test_acc_epoch), step=step)\n",
    "    return test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train size:  3991 124\n",
      "test size:  908 28\n"
     ]
    }
   ],
   "source": [
    "batchsize = 32\n",
    "blue = lambda x: '\\033[94m' + x + '\\033[0m'\n",
    "yellow = lambda x: '\\033[93m' + x + '\\033[0m'\n",
    "red = lambda x: '\\033[91m' + x + '\\033[0m'\n",
    "pre_transform, transform = T.NormalizeScale(), T.SamplePoints(1024)\n",
    "train_dataset = ModelNet('../data/modelnet10', '10', True, transform, pre_transform)\n",
    "test_dataset = ModelNet('../data/modelnet10', '10', False, transform, pre_transform)\n",
    "\n",
    "class Adapter:\n",
    "    def __init__(self, pg_dataset):\n",
    "        self.pg_dataset = pg_dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pg_dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.pg_dataset[idx].pos.numpy(), self.pg_dataset[idx].y.numpy().item() \n",
    "    \n",
    "print(\"train size: \", len(train_dataset), len(train_dataset)//batchsize)\n",
    "print(\"test size: \", len(test_dataset), len(test_dataset)//batchsize)\n",
    "train_loader = torch.utils.data.DataLoader(Adapter(train_dataset), batch_size=batchsize, shuffle=True, num_workers=0)\n",
    "test_loader  = torch.utils.data.DataLoader(Adapter(test_dataset), batch_size=batchsize, shuffle=False, num_workers=0)\n",
    "num2cat = dict(zip(range(10), train_dataset.raw_file_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from: models\\model_10_model_9.pth\n"
     ]
    }
   ],
   "source": [
    "num_classes = len(num2cat)\n",
    "num_batch = len(train_dataset)/batchsize\n",
    "\n",
    "classifier = PointNetCls(k=num_classes)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "classifier.to(device)\n",
    "optimizer = optim.Adam(classifier.parameters(), lr=0.001)    \n",
    "scheduler = StepLR(optimizer, step_size=1, gamma=0.99)\n",
    "\n",
    "model_path, model_epoch_cumulatiove_base = get_path_of_last_model()\n",
    "model_epoch_cumulatiove_base += 1\n",
    "if model_path:\n",
    "    print('Loading model from: {}'.format(model_path))\n",
    "    classifier.load_state_dict(torch.load(model_path))\n",
    "else:\n",
    "    print('No model dump used!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'bathtub',\n",
       " 1: 'bed',\n",
       " 2: 'chair',\n",
       " 3: 'desk',\n",
       " 4: 'dresser',\n",
       " 5: 'monitor',\n",
       " 6: 'night_stand',\n",
       " 7: 'sofa',\n",
       " 8: 'table',\n",
       " 9: 'toilet'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num2cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in test_loader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 908/908 [02:29<00:00,  5.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.22577092511013216\n"
     ]
    }
   ],
   "source": [
    "# points = data[0][0].unsqueeze(axis=0)\n",
    "# label = data[1][0]\n",
    "\n",
    "original = []\n",
    "predicted = []\n",
    "verbose = 0\n",
    "\n",
    "for i in tqdm(range(908)):\n",
    "    points = train_dataset[i]\n",
    "    label = train_dataset[i].y\n",
    "\n",
    "    pred_choice = infer(classifier, T.RandomRotate(uniform(45, 180), axis=0)(points).pos.unsqueeze(axis=0), label, num2cat, verbose)\n",
    "    infer(classifier, points.pos.unsqueeze(axis=0), label, num2cat, verbose)\n",
    "    original.append(label.item())\n",
    "    predicted.append(pred_choice.item())\n",
    "    if verbose:\n",
    "        print()\n",
    "        \n",
    "#     if label != pred_choice:\n",
    "#         view(points.pos)\n",
    "\n",
    "original = np.array(original) \n",
    "predicted = np.array(predicted) \n",
    "print((original==predicted).sum()/original.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.903\n",
    "0.907\n",
    "\n",
    "45-180 axis=0: 0.225\n",
    "\n",
    "\n",
    "0-5 axis=2: 0.911\n",
    "0-10 axis=2: 0.905\n",
    "45-180 axis=2: 0.506\n",
    "\n",
    "\n",
    "0-10 axis=1: 0.890\n",
    "0-20 axis=1: 0.866\n",
    "10-20 axis=1: 0.829\n",
    "10-20 axis=1: 0.853\n",
    "45-180 axis=1: 0.3381\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer(classifier, points, labels, num2cat, verbose=None):\n",
    "    points = points.transpose(2, 1)\n",
    "    points, labels = points.to(device), labels.to(device)\n",
    "    optimizer.zero_grad()\n",
    "    classifier = classifier.eval()\n",
    "    pred, _ = classifier(points)\n",
    "    pred = pred.view(-1, num_classes)\n",
    "    pred_choice = pred.data.max(1)[1]\n",
    "    if verbose:\n",
    "        print(\"predicted/real: {}/{}\".format(num2cat[pred_choice.item()], num2cat[labels.item()]))\n",
    "    return pred_choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bathtub 0 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pptk.viewer.viewer.viewer at 0x2944457b588>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(3000):\n",
    "    points, l = train_dataset[i].pos, train_dataset[i].y.item()\n",
    "    if l == 0:\n",
    "        break\n",
    "        \n",
    "print(num2cat[l], l, i)\n",
    "view(T.RandomRotate(0, axis=2)(train_dataset[i]).pos.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bathtub 0 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pptk.viewer.viewer.viewer at 0x294445717f0>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(3000):\n",
    "    points, l = train_dataset[i].pos, train_dataset[i].y.item()\n",
    "    if l == 0:\n",
    "        break\n",
    "        \n",
    "print(num2cat[l], l, i)\n",
    "view(T.RandomShear(0.5)(train_dataset[i]).pos.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bathtub 0 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pptk.viewer.viewer.viewer at 0x294445e1b00>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(3000):\n",
    "    points, l = train_dataset[i].pos, train_dataset[i].y.item()\n",
    "    if l == 0:\n",
    "        break\n",
    "        \n",
    "print(num2cat[l], l, i)\n",
    "view(T.RandomFlip(axis=1, p=1.0)(train_dataset[i]).pos.numpy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
