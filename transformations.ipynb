{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch_geometric.datasets import ModelNet\n",
    "import torch_geometric.transforms as T\n",
    "\n",
    "import mlflow\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch_geometric.nn import DynamicEdgeConv, global_max_pool\n",
    "\n",
    "from pointnet import PointNetCls\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_fscore_support, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pptk\n",
    "\n",
    "def view(points):\n",
    "    v = pptk.viewer(points)\n",
    "    v.attributes(points)\n",
    "    v.set(point_size=0.01)\n",
    "    # selected = points[v.get('selected')]\n",
    "    return v\n",
    "\n",
    "def plot_conf_matrix(conf_mtrx, labels, file_path='temp_conf_mtrx.png'):\n",
    "    df_cm = pd.DataFrame(np.array(conf_mtrx), index=labels, columns=labels)\n",
    "    plt.figure(figsize = (10,7))\n",
    "    sn.heatmap(df_cm, annot=True)\n",
    "    plt.savefig(file_path)\n",
    "    plt.close()\n",
    "    \n",
    "def get_path_of_last_model():\n",
    "    models_path = 'models'\n",
    "    files = list(filter(lambda f: os.path.isfile(os.path.join(models_path, f)) and f.endswith('.pth'), os.listdir(models_path) ))\n",
    "    if len(files) == 0:\n",
    "        return None, 0\n",
    "    files.sort(key=lambda f: int(f.split('.')[0].split('_')[-1] ))\n",
    "    return os.path.join(models_path, files[-1]), int(files[-1].split('.')[0].split('_')[-1])\n",
    "\n",
    "def test_model_full(classifier, test_data, num2cat, step=0, model_epoch_cumulatiove_base=0):\n",
    "    all_labels = []\n",
    "    all_choice = []\n",
    "    for j, data in enumerate(test_loader, 0):\n",
    "        points, labels = data\n",
    "        points = points.transpose(2, 1)\n",
    "        points, labels = points.to(device), labels.to(device)\n",
    "        classifier = classifier.eval()\n",
    "        with torch.no_grad():\n",
    "            pred, _ = classifier(points)\n",
    "        pred = pred.view(-1, num_classes)\n",
    "        loss = F.nll_loss(pred, labels)\n",
    "        pred_choice = pred.data.max(1)[1]\n",
    "            \n",
    "        all_labels.append(labels.cpu().numpy())\n",
    "        all_choice.append(pred_choice.cpu().numpy())\n",
    "            \n",
    "    all_labels = np.concatenate(all_labels)\n",
    "    all_choice = np.concatenate(all_choice)\n",
    "    test_acc = accuracy_score(all_labels, all_choice)\n",
    "    print(blue('epoch %d: %d/%d | test loss: %f | test acc: %f') % (model_epoch_cumulatiove_base+epoch+1, i+1, num_batch+1, loss.item(), test_acc))\n",
    "\n",
    "    cnf_mtrx = confusion_matrix(all_labels, all_choice, labels=sorted(list(num2cat)))\n",
    "    conf_mtrx_file_path = os.path.join(\"temp\", f\"test_cnf_mtrx_{epoch}_{i}.png\")\n",
    "    plot_conf_matrix(cnf_mtrx, [num2cat[num] for num in sorted(list(num2cat))], conf_mtrx_file_path)\n",
    "    mlflow.log_artifact(conf_mtrx_file_path)\n",
    "    mlflow.log_metric('test_acc', test_acc, step=step)\n",
    "    return test_acc\n",
    "    \n",
    "def test_model_simple(model, test_loader, step=0):\n",
    "    j, data = next(enumerate(test_loader, 0))\n",
    "    points, labels = data\n",
    "    points = points.transpose(2, 1)\n",
    "    points, labels = points.to(device), labels.to(device)\n",
    "    classifier = classifier.eval()\n",
    "    with torch.no_grad():\n",
    "        pred, _ = classifier(points)\n",
    "    pred = pred.view(-1, num_classes)\n",
    "    loss = F.nll_loss(pred, labels)\n",
    "    pred_choice = pred.data.max(1)[1]\n",
    "    correct = pred_choice.eq(labels.data).cpu().sum()\n",
    "    test_acc = correct.item() / float(batchsize)\n",
    "    print(blue('epoch %d: %d/%d | test loss: %f | test acc: %f') % (model_epoch_cumulatiove_base+epoch+1, i+1, num_batch+1, loss.item(), test_acc))\n",
    "\n",
    "    # log test\n",
    "    cnf_mtrx = confusion_matrix(labels.cpu().tolist(), pred_choice.cpu().tolist(), labels=sorted(list(num2cat)))\n",
    "    conf_mtrx_file_path = os.path.join(\"temp\", f\"test_cnf_mtrx_{epoch}_{i}.png\")\n",
    "    plot_conf_matrix(cnf_mtrx, [num2cat[num] for num in sorted(list(num2cat))], conf_mtrx_file_path)\n",
    "    mlflow.log_artifact(conf_mtrx_file_path)\n",
    "    mlflow.log_metric('test_acc', np.mean(test_acc_epoch), step=step)\n",
    "    return test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train size:  3991 124\n",
      "test size:  908 28\n"
     ]
    }
   ],
   "source": [
    "batchsize = 32\n",
    "blue = lambda x: '\\033[94m' + x + '\\033[0m'\n",
    "yellow = lambda x: '\\033[93m' + x + '\\033[0m'\n",
    "red = lambda x: '\\033[91m' + x + '\\033[0m'\n",
    "pre_transform, transform = T.NormalizeScale(), T.SamplePoints(1024)\n",
    "train_dataset = ModelNet('../data/modelnet10', '10', True, transform, pre_transform)\n",
    "test_dataset = ModelNet('../data/modelnet10', '10', False, transform, pre_transform)\n",
    "\n",
    "class Adapter:\n",
    "    def __init__(self, pg_dataset):\n",
    "        self.pg_dataset = pg_dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pg_dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.pg_dataset[idx].pos.numpy(), self.pg_dataset[idx].y.numpy().item() \n",
    "    \n",
    "print(\"train size: \", len(train_dataset), len(train_dataset)//batchsize)\n",
    "print(\"test size: \", len(test_dataset), len(test_dataset)//batchsize)\n",
    "train_loader = torch.utils.data.DataLoader(Adapter(train_dataset), batch_size=batchsize, shuffle=True, num_workers=0)\n",
    "test_loader  = torch.utils.data.DataLoader(Adapter(test_dataset), batch_size=batchsize, shuffle=False, num_workers=0)\n",
    "num2cat = dict(zip(range(10), train_dataset.raw_file_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bathtub 0 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pptk.viewer.viewer.viewer at 0x2944457b588>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(3000):\n",
    "    points, l = train_dataset[i].pos, train_dataset[i].y.item()\n",
    "    if l == 0:\n",
    "        break\n",
    "        \n",
    "print(num2cat[l], l, i)\n",
    "view(T.RandomRotate(0, axis=2)(train_dataset[i]).pos.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bathtub 0 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pptk.viewer.viewer.viewer at 0x294445717f0>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(3000):\n",
    "    points, l = train_dataset[i].pos, train_dataset[i].y.item()\n",
    "    if l == 0:\n",
    "        break\n",
    "        \n",
    "print(num2cat[l], l, i)\n",
    "view(T.RandomShear(0.5)(train_dataset[i]).pos.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bathtub 0 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pptk.viewer.viewer.viewer at 0x294445e1b00>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(3000):\n",
    "    points, l = train_dataset[i].pos, train_dataset[i].y.item()\n",
    "    if l == 0:\n",
    "        break\n",
    "        \n",
    "print(num2cat[l], l, i)\n",
    "view(T.RandomFlip(axis=1, p=1.0)(train_dataset[i]).pos.numpy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
