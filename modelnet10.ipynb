{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch_geometric.datasets import ModelNet\n",
    "import torch_geometric.transforms as T\n",
    "\n",
    "import mlflow\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch_geometric.nn import DynamicEdgeConv, global_max_pool\n",
    "\n",
    "from pointnet import PointNetCls\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_fscore_support, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pptk\n",
    "\n",
    "def view(points):\n",
    "    v = pptk.viewer(points)\n",
    "    v.attributes(points)\n",
    "    v.set(point_size=0.01)\n",
    "    # selected = points[v.get('selected')]\n",
    "    return v\n",
    "\n",
    "def plot_conf_matrix(conf_mtrx, labels, file_path='temp_conf_mtrx.png'):\n",
    "    df_cm = pd.DataFrame(np.array(conf_mtrx), index=labels, columns=labels)\n",
    "    plt.figure(figsize = (10,7))\n",
    "    sn.heatmap(df_cm, annot=True)\n",
    "    plt.savefig(file_path)\n",
    "    plt.close()\n",
    "    \n",
    "def get_path_of_last_model():\n",
    "    models_path = 'models'\n",
    "    files = list(filter(lambda f: os.path.isfile(os.path.join(models_path, f)) and f.endswith('.pth'), os.listdir(models_path) ))\n",
    "    if len(files) == 0:\n",
    "        return None, 0\n",
    "    files.sort(key=lambda f: int(f.split('.')[0].split('_')[-1] ))\n",
    "    return os.path.join(models_path, files[-1]), int(files[-1].split('.')[0].split('_')[-1])\n",
    "\n",
    "def test_model_full(classifier, test_data, num2cat, step=0, model_epoch_cumulatiove_base=0):\n",
    "    all_labels = []\n",
    "    all_choice = []\n",
    "    for j, data in enumerate(test_loader, 0):\n",
    "        points, labels = data\n",
    "        points = points.transpose(2, 1)\n",
    "        points, labels = points.to(device), labels.to(device)\n",
    "        classifier = classifier.eval()\n",
    "        with torch.no_grad():\n",
    "            pred, _ = classifier(points)\n",
    "        pred = pred.view(-1, num_classes)\n",
    "        loss = F.nll_loss(pred, labels)\n",
    "        pred_choice = pred.data.max(1)[1]\n",
    "            \n",
    "        all_labels.append(labels.cpu().numpy())\n",
    "        all_choice.append(pred_choice.cpu().numpy())\n",
    "            \n",
    "    all_labels = np.concatenate(all_labels)\n",
    "    all_choice = np.concatenate(all_choice)\n",
    "    test_acc = accuracy_score(all_labels, all_choice)\n",
    "    print(blue('epoch %d: %d/%d | test loss: %f | test acc: %f') % (model_epoch_cumulatiove_base+epoch+1, i+1, num_batch+1, loss.item(), test_acc))\n",
    "\n",
    "    cnf_mtrx = confusion_matrix(all_labels, all_choice, labels=sorted(list(num2cat)))\n",
    "    conf_mtrx_file_path = os.path.join(\"temp\", f\"test_cnf_mtrx_{epoch}_{i}.png\")\n",
    "    plot_conf_matrix(cnf_mtrx, [num2cat[num] for num in sorted(list(num2cat))], conf_mtrx_file_path)\n",
    "    mlflow.log_artifact(conf_mtrx_file_path)\n",
    "    mlflow.log_metric('test_acc', test_acc, step=step)\n",
    "    return test_acc\n",
    "    \n",
    "def test_model_simple(model, test_loader, step=0):\n",
    "    j, data = next(enumerate(test_loader, 0))\n",
    "    points, labels = data\n",
    "    points = points.transpose(2, 1)\n",
    "    points, labels = points.to(device), labels.to(device)\n",
    "    classifier = classifier.eval()\n",
    "    with torch.no_grad():\n",
    "        pred, _ = classifier(points)\n",
    "    pred = pred.view(-1, num_classes)\n",
    "    loss = F.nll_loss(pred, labels)\n",
    "    pred_choice = pred.data.max(1)[1]\n",
    "    correct = pred_choice.eq(labels.data).cpu().sum()\n",
    "    test_acc = correct.item() / float(batchsize)\n",
    "    print(blue('epoch %d: %d/%d | test loss: %f | test acc: %f') % (model_epoch_cumulatiove_base+epoch+1, i+1, num_batch+1, loss.item(), test_acc))\n",
    "\n",
    "    # log test\n",
    "    cnf_mtrx = confusion_matrix(labels.cpu().tolist(), pred_choice.cpu().tolist(), labels=sorted(list(num2cat)))\n",
    "    conf_mtrx_file_path = os.path.join(\"temp\", f\"test_cnf_mtrx_{epoch}_{i}.png\")\n",
    "    plot_conf_matrix(cnf_mtrx, [num2cat[num] for num in sorted(list(num2cat))], conf_mtrx_file_path)\n",
    "    mlflow.log_artifact(conf_mtrx_file_path)\n",
    "    mlflow.log_metric('test_acc', np.mean(test_acc_epoch), step=step)\n",
    "    return test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train size:  3991 124\n",
      "test size:  908 28\n"
     ]
    }
   ],
   "source": [
    "batchsize = 32\n",
    "blue = lambda x: '\\033[94m' + x + '\\033[0m'\n",
    "yellow = lambda x: '\\033[93m' + x + '\\033[0m'\n",
    "red = lambda x: '\\033[91m' + x + '\\033[0m'\n",
    "pre_transform, transform = T.NormalizeScale(), T.SamplePoints(1024)\n",
    "train_dataset = ModelNet('../data/modelnet10', '10', True, transform, pre_transform)\n",
    "test_dataset = ModelNet('../data/modelnet10', '10', False, transform, pre_transform)\n",
    "\n",
    "class Adapter:\n",
    "    def __init__(self, pg_dataset):\n",
    "        self.pg_dataset = pg_dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pg_dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.pg_dataset[idx].pos.numpy(), self.pg_dataset[idx].y.numpy().item() \n",
    "    \n",
    "print(\"train size: \", len(train_dataset), len(train_dataset)//batchsize)\n",
    "print(\"test size: \", len(test_dataset), len(test_dataset)//batchsize)\n",
    "train_loader = torch.utils.data.DataLoader(Adapter(train_dataset), batch_size=batchsize, shuffle=True, num_workers=0)\n",
    "test_loader  = torch.utils.data.DataLoader(Adapter(test_dataset), batch_size=batchsize, shuffle=False, num_workers=0)\n",
    "num2cat = dict(zip(range(10), train_dataset.raw_file_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from: models\\model_10_model_9.pth\n"
     ]
    }
   ],
   "source": [
    "num_classes = len(num2cat)\n",
    "num_batch = len(train_dataset)/batchsize\n",
    "\n",
    "classifier = PointNetCls(k=num_classes)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "classifier.to(device)\n",
    "optimizer = optim.Adam(classifier.parameters(), lr=0.001)    \n",
    "scheduler = StepLR(optimizer, step_size=1, gamma=0.99)\n",
    "\n",
    "model_path, model_epoch_cumulatiove_base = get_path_of_last_model()\n",
    "model_epoch_cumulatiove_base += 1\n",
    "if model_path:\n",
    "    print('Loading model from: {}'.format(model_path))\n",
    "    mlflow.log_param('start', model_path)\n",
    "    classifier.load_state_dict(torch.load(model_path))\n",
    "else:\n",
    "    print('Start training from zero!')\n",
    "    mlflow.log_param('start', 'From zero')\n",
    "\n",
    "mlflow.log_param('start epoch', model_epoch_cumulatiove_base)\n",
    "step = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 11: 1/125 | train loss: 0.275583 | train acc: 0.875000\n",
      "epoch 11: 2/125 | train loss: 0.439769 | train acc: 0.781250\n",
      "epoch 11: 3/125 | train loss: 0.345226 | train acc: 0.875000\n",
      "epoch 11: 4/125 | train loss: 0.118115 | train acc: 0.968750\n",
      "epoch 11: 5/125 | train loss: 0.686675 | train acc: 0.718750\n",
      "epoch 11: 6/125 | train loss: 0.242373 | train acc: 0.906250\n",
      "epoch 11: 7/125 | train loss: 0.432911 | train acc: 0.843750\n",
      "epoch 11: 8/125 | train loss: 0.722645 | train acc: 0.718750\n",
      "epoch 11: 9/125 | train loss: 0.386642 | train acc: 0.843750\n",
      "epoch 11: 10/125 | train loss: 0.317425 | train acc: 0.937500\n",
      "epoch 11: 10/125 | test loss: 0.122314 | test acc: 0.806167\n",
      "epoch 11: 11/125 | train loss: 0.516232 | train acc: 0.750000\n",
      "epoch 11: 12/125 | train loss: 0.597246 | train acc: 0.843750\n",
      "epoch 11: 13/125 | train loss: 0.390845 | train acc: 0.843750\n",
      "epoch 11: 14/125 | train loss: 0.227128 | train acc: 0.906250\n",
      "epoch 11: 15/125 | train loss: 0.135304 | train acc: 1.000000\n",
      "epoch 11: 16/125 | train loss: 0.282365 | train acc: 0.906250\n",
      "epoch 11: 17/125 | train loss: 0.516644 | train acc: 0.843750\n",
      "epoch 11: 18/125 | train loss: 0.423397 | train acc: 0.906250\n",
      "epoch 11: 19/125 | train loss: 0.359961 | train acc: 0.843750\n",
      "epoch 11: 20/125 | train loss: 0.358083 | train acc: 0.875000\n",
      "epoch 11: 20/125 | test loss: 0.008917 | test acc: 0.789648\n",
      "epoch 11: 21/125 | train loss: 0.703008 | train acc: 0.718750\n",
      "epoch 11: 22/125 | train loss: 0.207002 | train acc: 0.937500\n",
      "epoch 11: 23/125 | train loss: 0.283344 | train acc: 0.875000\n",
      "epoch 11: 24/125 | train loss: 0.355216 | train acc: 0.843750\n",
      "epoch 11: 25/125 | train loss: 0.369927 | train acc: 0.843750\n",
      "epoch 11: 26/125 | train loss: 0.200808 | train acc: 0.906250\n",
      "epoch 11: 27/125 | train loss: 0.633768 | train acc: 0.750000\n",
      "epoch 11: 28/125 | train loss: 0.579439 | train acc: 0.781250\n",
      "epoch 11: 29/125 | train loss: 0.125374 | train acc: 0.937500\n",
      "epoch 11: 30/125 | train loss: 0.350984 | train acc: 0.906250\n",
      "epoch 11: 30/125 | test loss: 0.014591 | test acc: 0.805066\n",
      "epoch 11: 31/125 | train loss: 0.422255 | train acc: 0.812500\n",
      "epoch 11: 32/125 | train loss: 0.588779 | train acc: 0.843750\n",
      "epoch 11: 33/125 | train loss: 0.108410 | train acc: 0.968750\n",
      "epoch 11: 34/125 | train loss: 0.508406 | train acc: 0.843750\n",
      "epoch 11: 35/125 | train loss: 0.285338 | train acc: 0.875000\n",
      "epoch 11: 36/125 | train loss: 0.592466 | train acc: 0.750000\n",
      "epoch 11: 37/125 | train loss: 0.259333 | train acc: 0.937500\n",
      "epoch 11: 38/125 | train loss: 0.274690 | train acc: 0.937500\n",
      "epoch 11: 39/125 | train loss: 0.546186 | train acc: 0.843750\n",
      "epoch 11: 40/125 | train loss: 0.394875 | train acc: 0.843750\n",
      "epoch 11: 40/125 | test loss: 0.007646 | test acc: 0.802863\n",
      "epoch 11: 41/125 | train loss: 0.200887 | train acc: 0.906250\n",
      "epoch 11: 42/125 | train loss: 0.261499 | train acc: 0.906250\n",
      "epoch 11: 43/125 | train loss: 0.253974 | train acc: 0.906250\n",
      "epoch 11: 44/125 | train loss: 0.474284 | train acc: 0.812500\n",
      "epoch 11: 45/125 | train loss: 0.385562 | train acc: 0.812500\n",
      "epoch 11: 46/125 | train loss: 0.321136 | train acc: 0.937500\n",
      "epoch 11: 47/125 | train loss: 0.163786 | train acc: 0.937500\n",
      "epoch 11: 48/125 | train loss: 0.435994 | train acc: 0.843750\n",
      "epoch 11: 49/125 | train loss: 0.186840 | train acc: 0.906250\n",
      "epoch 11: 50/125 | train loss: 0.744116 | train acc: 0.781250\n",
      "epoch 11: 50/125 | test loss: 0.006818 | test acc: 0.808370\n",
      "epoch 11: 51/125 | train loss: 0.267270 | train acc: 0.968750\n",
      "epoch 11: 52/125 | train loss: 0.568900 | train acc: 0.812500\n",
      "epoch 11: 53/125 | train loss: 0.210745 | train acc: 0.937500\n",
      "epoch 11: 54/125 | train loss: 0.416777 | train acc: 0.781250\n",
      "epoch 11: 55/125 | train loss: 0.453714 | train acc: 0.812500\n",
      "epoch 11: 56/125 | train loss: 0.275233 | train acc: 0.937500\n",
      "epoch 11: 57/125 | train loss: 0.393258 | train acc: 0.875000\n",
      "epoch 11: 58/125 | train loss: 0.235417 | train acc: 0.875000\n",
      "epoch 11: 59/125 | train loss: 0.247982 | train acc: 0.906250\n",
      "epoch 11: 60/125 | train loss: 0.548234 | train acc: 0.843750\n",
      "epoch 11: 60/125 | test loss: 0.016589 | test acc: 0.818282\n",
      "epoch 11: 61/125 | train loss: 0.744852 | train acc: 0.687500\n",
      "epoch 11: 62/125 | train loss: 0.161233 | train acc: 0.968750\n",
      "epoch 11: 63/125 | train loss: 0.260133 | train acc: 0.968750\n",
      "epoch 11: 64/125 | train loss: 0.247221 | train acc: 0.906250\n",
      "epoch 11: 65/125 | train loss: 0.288841 | train acc: 0.843750\n",
      "epoch 11: 66/125 | train loss: 0.471224 | train acc: 0.875000\n",
      "epoch 11: 67/125 | train loss: 0.533132 | train acc: 0.781250\n",
      "epoch 11: 68/125 | train loss: 0.374947 | train acc: 0.906250\n",
      "epoch 11: 69/125 | train loss: 0.387318 | train acc: 0.906250\n",
      "epoch 11: 70/125 | train loss: 0.408815 | train acc: 0.906250\n",
      "epoch 11: 70/125 | test loss: 0.034173 | test acc: 0.838106\n",
      "epoch 11: 71/125 | train loss: 0.279115 | train acc: 0.843750\n",
      "epoch 11: 72/125 | train loss: 0.249339 | train acc: 0.937500\n",
      "epoch 11: 73/125 | train loss: 0.225892 | train acc: 0.906250\n",
      "epoch 11: 74/125 | train loss: 0.358801 | train acc: 0.906250\n",
      "epoch 11: 75/125 | train loss: 0.396281 | train acc: 0.812500\n",
      "epoch 11: 76/125 | train loss: 0.422180 | train acc: 0.875000\n",
      "epoch 11: 77/125 | train loss: 0.081905 | train acc: 1.000000\n",
      "epoch 11: 78/125 | train loss: 0.385822 | train acc: 0.843750\n",
      "epoch 11: 79/125 | train loss: 0.199353 | train acc: 0.937500\n",
      "epoch 11: 80/125 | train loss: 0.219584 | train acc: 0.906250\n",
      "epoch 11: 80/125 | test loss: 0.085733 | test acc: 0.843612\n",
      "epoch 11: 81/125 | train loss: 0.233212 | train acc: 0.937500\n",
      "epoch 11: 82/125 | train loss: 0.256191 | train acc: 0.906250\n",
      "epoch 11: 83/125 | train loss: 0.369422 | train acc: 0.843750\n",
      "epoch 11: 84/125 | train loss: 0.449668 | train acc: 0.812500\n",
      "epoch 11: 85/125 | train loss: 0.235777 | train acc: 0.875000\n",
      "epoch 11: 86/125 | train loss: 0.365021 | train acc: 0.875000\n",
      "epoch 11: 87/125 | train loss: 0.139504 | train acc: 0.968750\n",
      "epoch 11: 88/125 | train loss: 0.272780 | train acc: 0.937500\n",
      "epoch 11: 89/125 | train loss: 0.460548 | train acc: 0.781250\n",
      "epoch 11: 90/125 | train loss: 0.208845 | train acc: 0.937500\n",
      "epoch 11: 90/125 | test loss: 0.114113 | test acc: 0.845815\n",
      "epoch 11: 91/125 | train loss: 0.202973 | train acc: 0.906250\n",
      "epoch 11: 92/125 | train loss: 0.462740 | train acc: 0.843750\n",
      "epoch 11: 93/125 | train loss: 0.685312 | train acc: 0.843750\n",
      "epoch 11: 94/125 | train loss: 0.219813 | train acc: 0.937500\n",
      "epoch 11: 95/125 | train loss: 0.409806 | train acc: 0.843750\n",
      "epoch 11: 96/125 | train loss: 0.351141 | train acc: 0.843750\n",
      "epoch 11: 97/125 | train loss: 0.312719 | train acc: 0.906250\n",
      "epoch 11: 98/125 | train loss: 0.230449 | train acc: 0.937500\n",
      "epoch 11: 99/125 | train loss: 0.495241 | train acc: 0.812500\n",
      "epoch 11: 100/125 | train loss: 0.391541 | train acc: 0.875000\n",
      "epoch 11: 100/125 | test loss: 0.036855 | test acc: 0.852423\n",
      "epoch 11: 101/125 | train loss: 0.157238 | train acc: 0.968750\n",
      "epoch 11: 102/125 | train loss: 0.695401 | train acc: 0.750000\n",
      "epoch 11: 103/125 | train loss: 0.123527 | train acc: 0.968750\n",
      "epoch 11: 104/125 | train loss: 0.169022 | train acc: 0.968750\n",
      "epoch 11: 105/125 | train loss: 0.493820 | train acc: 0.875000\n",
      "epoch 11: 106/125 | train loss: 0.291324 | train acc: 0.875000\n",
      "epoch 11: 107/125 | train loss: 0.171683 | train acc: 0.968750\n",
      "epoch 11: 108/125 | train loss: 0.248211 | train acc: 0.906250\n",
      "epoch 11: 109/125 | train loss: 0.463544 | train acc: 0.906250\n",
      "epoch 11: 110/125 | train loss: 0.317561 | train acc: 0.843750\n",
      "epoch 11: 110/125 | test loss: 0.025427 | test acc: 0.843612\n",
      "epoch 11: 111/125 | train loss: 0.431359 | train acc: 0.906250\n",
      "epoch 11: 112/125 | train loss: 0.192534 | train acc: 0.937500\n",
      "epoch 11: 113/125 | train loss: 0.292961 | train acc: 0.906250\n",
      "epoch 11: 114/125 | train loss: 0.240651 | train acc: 0.906250\n",
      "epoch 11: 115/125 | train loss: 0.370843 | train acc: 0.875000\n",
      "epoch 11: 116/125 | train loss: 0.120020 | train acc: 0.968750\n",
      "epoch 11: 117/125 | train loss: 0.215515 | train acc: 0.937500\n",
      "epoch 11: 118/125 | train loss: 0.243535 | train acc: 0.875000\n",
      "epoch 11: 119/125 | train loss: 0.185696 | train acc: 0.937500\n",
      "epoch 11: 120/125 | train loss: 0.169592 | train acc: 0.937500\n",
      "epoch 11: 120/125 | test loss: 0.027828 | test acc: 0.855727\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 11: 121/125 | train loss: 0.209844 | train acc: 0.937500\n",
      "epoch 11: 122/125 | train loss: 0.255058 | train acc: 0.843750\n",
      "epoch 11: 123/125 | train loss: 0.373178 | train acc: 0.875000\n",
      "epoch 11: 124/125 | train loss: 0.358593 | train acc: 0.843750\n",
      "epoch 11: 125/125 | train loss: 0.223243 | train acc: 0.625000\n",
      "epoch 11 | mean train acc: 0.876250\n",
      "epoch 11 | mean test acc: 0.825808\n",
      "epoch 12: 1/125 | train loss: 0.226599 | train acc: 0.937500\n",
      "epoch 12: 2/125 | train loss: 0.355719 | train acc: 0.875000\n",
      "epoch 12: 3/125 | train loss: 0.135115 | train acc: 0.968750\n",
      "epoch 12: 4/125 | train loss: 0.226976 | train acc: 0.937500\n",
      "epoch 12: 5/125 | train loss: 0.162746 | train acc: 0.968750\n",
      "epoch 12: 6/125 | train loss: 0.300707 | train acc: 0.906250\n",
      "epoch 12: 7/125 | train loss: 0.400740 | train acc: 0.906250\n",
      "epoch 12: 8/125 | train loss: 0.198019 | train acc: 0.906250\n",
      "epoch 12: 9/125 | train loss: 0.218673 | train acc: 0.875000\n",
      "epoch 12: 10/125 | train loss: 0.248391 | train acc: 0.906250\n",
      "epoch 12: 10/125 | test loss: 0.020199 | test acc: 0.853524\n",
      "epoch 12: 11/125 | train loss: 0.236857 | train acc: 0.906250\n",
      "epoch 12: 12/125 | train loss: 0.534448 | train acc: 0.843750\n",
      "epoch 12: 13/125 | train loss: 0.173714 | train acc: 0.968750\n",
      "epoch 12: 14/125 | train loss: 0.146980 | train acc: 0.968750\n",
      "epoch 12: 15/125 | train loss: 0.376875 | train acc: 0.843750\n",
      "epoch 12: 16/125 | train loss: 0.357612 | train acc: 0.875000\n",
      "epoch 12: 17/125 | train loss: 0.158542 | train acc: 0.937500\n",
      "epoch 12: 18/125 | train loss: 0.287664 | train acc: 0.968750\n",
      "epoch 12: 19/125 | train loss: 0.735458 | train acc: 0.812500\n",
      "epoch 12: 20/125 | train loss: 0.275795 | train acc: 0.937500\n",
      "epoch 12: 20/125 | test loss: 0.024319 | test acc: 0.859031\n",
      "epoch 12: 21/125 | train loss: 0.398167 | train acc: 0.843750\n",
      "epoch 12: 22/125 | train loss: 0.268682 | train acc: 0.937500\n",
      "epoch 12: 23/125 | train loss: 0.163926 | train acc: 0.937500\n",
      "epoch 12: 24/125 | train loss: 0.176919 | train acc: 0.937500\n",
      "epoch 12: 25/125 | train loss: 0.240298 | train acc: 0.906250\n",
      "epoch 12: 26/125 | train loss: 0.203436 | train acc: 0.875000\n",
      "epoch 12: 27/125 | train loss: 0.169262 | train acc: 0.937500\n",
      "epoch 12: 28/125 | train loss: 0.678083 | train acc: 0.875000\n",
      "epoch 12: 29/125 | train loss: 0.271608 | train acc: 0.906250\n",
      "epoch 12: 30/125 | train loss: 0.356627 | train acc: 0.875000\n",
      "epoch 12: 30/125 | test loss: 0.012332 | test acc: 0.852423\n",
      "epoch 12: 31/125 | train loss: 0.264115 | train acc: 0.906250\n",
      "epoch 12: 32/125 | train loss: 0.172980 | train acc: 0.906250\n",
      "epoch 12: 33/125 | train loss: 0.320750 | train acc: 0.812500\n",
      "epoch 12: 34/125 | train loss: 0.288721 | train acc: 0.843750\n",
      "epoch 12: 35/125 | train loss: 0.459468 | train acc: 0.843750\n",
      "epoch 12: 36/125 | train loss: 0.157444 | train acc: 0.937500\n",
      "epoch 12: 37/125 | train loss: 0.423834 | train acc: 0.843750\n",
      "epoch 12: 38/125 | train loss: 0.279666 | train acc: 0.843750\n",
      "epoch 12: 39/125 | train loss: 0.307256 | train acc: 0.875000\n",
      "epoch 12: 40/125 | train loss: 0.226219 | train acc: 0.937500\n",
      "epoch 12: 40/125 | test loss: 0.010613 | test acc: 0.846916\n",
      "epoch 12: 41/125 | train loss: 0.250057 | train acc: 0.937500\n",
      "epoch 12: 42/125 | train loss: 0.328035 | train acc: 0.875000\n",
      "epoch 12: 43/125 | train loss: 0.245343 | train acc: 0.906250\n",
      "epoch 12: 44/125 | train loss: 0.245919 | train acc: 0.875000\n",
      "epoch 12: 45/125 | train loss: 0.318012 | train acc: 0.875000\n",
      "epoch 12: 46/125 | train loss: 0.380952 | train acc: 0.875000\n",
      "epoch 12: 47/125 | train loss: 0.274882 | train acc: 0.906250\n",
      "epoch 12: 48/125 | train loss: 0.118239 | train acc: 1.000000\n",
      "epoch 12: 49/125 | train loss: 0.114697 | train acc: 0.968750\n",
      "epoch 12: 50/125 | train loss: 0.210705 | train acc: 0.937500\n",
      "epoch 12: 50/125 | test loss: 0.045796 | test acc: 0.853524\n",
      "epoch 12: 51/125 | train loss: 0.176943 | train acc: 0.875000\n",
      "epoch 12: 52/125 | train loss: 0.304539 | train acc: 0.906250\n",
      "epoch 12: 53/125 | train loss: 0.157038 | train acc: 0.937500\n",
      "epoch 12: 54/125 | train loss: 0.229299 | train acc: 0.937500\n",
      "epoch 12: 55/125 | train loss: 0.173826 | train acc: 0.968750\n",
      "epoch 12: 56/125 | train loss: 0.410837 | train acc: 0.875000\n",
      "epoch 12: 57/125 | train loss: 0.243046 | train acc: 0.937500\n",
      "epoch 12: 58/125 | train loss: 0.467819 | train acc: 0.843750\n",
      "epoch 12: 59/125 | train loss: 0.430471 | train acc: 0.843750\n",
      "epoch 12: 60/125 | train loss: 0.103234 | train acc: 0.968750\n",
      "epoch 12: 60/125 | test loss: 0.071499 | test acc: 0.860132\n",
      "epoch 12: 61/125 | train loss: 0.362165 | train acc: 0.843750\n",
      "epoch 12: 62/125 | train loss: 0.208827 | train acc: 0.937500\n",
      "epoch 12: 63/125 | train loss: 0.262205 | train acc: 0.906250\n",
      "epoch 12: 64/125 | train loss: 0.141244 | train acc: 0.937500\n",
      "epoch 12: 65/125 | train loss: 0.207565 | train acc: 0.937500\n",
      "epoch 12: 66/125 | train loss: 0.394031 | train acc: 0.875000\n",
      "epoch 12: 67/125 | train loss: 0.268928 | train acc: 0.875000\n",
      "epoch 12: 68/125 | train loss: 0.368867 | train acc: 0.906250\n",
      "epoch 12: 69/125 | train loss: 0.243787 | train acc: 0.937500\n",
      "epoch 12: 70/125 | train loss: 0.275720 | train acc: 0.875000\n",
      "epoch 12: 70/125 | test loss: 0.047635 | test acc: 0.860132\n",
      "epoch 12: 71/125 | train loss: 0.160350 | train acc: 0.937500\n",
      "epoch 12: 72/125 | train loss: 0.165662 | train acc: 0.937500\n",
      "epoch 12: 73/125 | train loss: 0.165299 | train acc: 0.937500\n",
      "epoch 12: 74/125 | train loss: 0.207872 | train acc: 0.968750\n",
      "epoch 12: 75/125 | train loss: 0.151252 | train acc: 0.937500\n",
      "epoch 12: 76/125 | train loss: 0.250944 | train acc: 0.875000\n",
      "epoch 12: 77/125 | train loss: 0.465122 | train acc: 0.875000\n",
      "epoch 12: 78/125 | train loss: 0.260322 | train acc: 0.843750\n",
      "epoch 12: 79/125 | train loss: 0.302224 | train acc: 0.937500\n",
      "epoch 12: 80/125 | train loss: 0.289367 | train acc: 0.875000\n",
      "epoch 12: 80/125 | test loss: 0.014879 | test acc: 0.865639\n",
      "epoch 12: 81/125 | train loss: 0.342291 | train acc: 0.875000\n",
      "epoch 12: 82/125 | train loss: 0.340060 | train acc: 0.812500\n",
      "epoch 12: 83/125 | train loss: 0.090879 | train acc: 1.000000\n",
      "epoch 12: 84/125 | train loss: 0.225705 | train acc: 0.968750\n",
      "epoch 12: 85/125 | train loss: 0.124733 | train acc: 0.968750\n",
      "epoch 12: 86/125 | train loss: 0.151771 | train acc: 0.968750\n",
      "epoch 12: 87/125 | train loss: 0.224964 | train acc: 0.937500\n",
      "epoch 12: 88/125 | train loss: 0.255623 | train acc: 0.937500\n",
      "epoch 12: 89/125 | train loss: 0.072274 | train acc: 1.000000\n",
      "epoch 12: 90/125 | train loss: 0.087344 | train acc: 0.968750\n",
      "epoch 12: 90/125 | test loss: 0.024093 | test acc: 0.856828\n",
      "epoch 12: 91/125 | train loss: 0.188278 | train acc: 0.968750\n",
      "epoch 12: 92/125 | train loss: 0.205257 | train acc: 0.875000\n",
      "epoch 12: 93/125 | train loss: 0.389612 | train acc: 0.875000\n",
      "epoch 12: 94/125 | train loss: 0.619199 | train acc: 0.750000\n",
      "epoch 12: 95/125 | train loss: 0.236502 | train acc: 0.906250\n",
      "epoch 12: 96/125 | train loss: 0.090043 | train acc: 0.968750\n",
      "epoch 12: 97/125 | train loss: 0.233663 | train acc: 0.937500\n",
      "epoch 12: 98/125 | train loss: 0.190106 | train acc: 0.906250\n",
      "epoch 12: 99/125 | train loss: 0.274777 | train acc: 0.906250\n",
      "epoch 12: 100/125 | train loss: 0.154013 | train acc: 0.937500\n",
      "epoch 12: 100/125 | test loss: 0.042628 | test acc: 0.866740\n",
      "epoch 12: 101/125 | train loss: 0.097575 | train acc: 1.000000\n",
      "epoch 12: 102/125 | train loss: 0.123767 | train acc: 0.968750\n",
      "epoch 12: 103/125 | train loss: 0.182525 | train acc: 0.906250\n",
      "epoch 12: 104/125 | train loss: 0.190410 | train acc: 0.906250\n",
      "epoch 12: 105/125 | train loss: 0.188137 | train acc: 0.937500\n",
      "epoch 12: 106/125 | train loss: 0.194882 | train acc: 0.968750\n",
      "epoch 12: 107/125 | train loss: 0.266629 | train acc: 0.875000\n",
      "epoch 12: 108/125 | train loss: 0.295708 | train acc: 0.906250\n",
      "epoch 12: 109/125 | train loss: 0.112129 | train acc: 0.968750\n",
      "epoch 12: 110/125 | train loss: 0.261249 | train acc: 0.906250\n",
      "epoch 12: 110/125 | test loss: 0.010064 | test acc: 0.862335\n",
      "epoch 12: 111/125 | train loss: 0.249018 | train acc: 0.875000\n",
      "epoch 12: 112/125 | train loss: 0.450644 | train acc: 0.812500\n",
      "epoch 12: 113/125 | train loss: 0.227629 | train acc: 0.875000\n",
      "epoch 12: 114/125 | train loss: 0.492511 | train acc: 0.875000\n",
      "epoch 12: 115/125 | train loss: 0.054029 | train acc: 1.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 12: 116/125 | train loss: 0.167663 | train acc: 0.906250\n",
      "epoch 12: 117/125 | train loss: 0.435545 | train acc: 0.843750\n",
      "epoch 12: 118/125 | train loss: 0.426009 | train acc: 0.906250\n",
      "epoch 12: 119/125 | train loss: 0.346382 | train acc: 0.906250\n",
      "epoch 12: 120/125 | train loss: 0.077924 | train acc: 1.000000\n",
      "epoch 12: 120/125 | test loss: 0.036251 | test acc: 0.872247\n",
      "epoch 12: 121/125 | train loss: 0.373868 | train acc: 0.937500\n",
      "epoch 12: 122/125 | train loss: 0.195024 | train acc: 0.906250\n",
      "epoch 12: 123/125 | train loss: 0.114135 | train acc: 0.968750\n",
      "epoch 12: 124/125 | train loss: 0.229346 | train acc: 0.906250\n",
      "epoch 12: 125/125 | train loss: 0.492783 | train acc: 0.625000\n",
      "epoch 12 | mean train acc: 0.908750\n",
      "epoch 12 | mean test acc: 0.859123\n",
      "epoch 13: 1/125 | train loss: 0.308717 | train acc: 0.843750\n",
      "epoch 13: 2/125 | train loss: 0.417101 | train acc: 0.781250\n",
      "epoch 13: 3/125 | train loss: 0.324426 | train acc: 0.906250\n",
      "epoch 13: 4/125 | train loss: 0.239396 | train acc: 0.968750\n",
      "epoch 13: 5/125 | train loss: 0.121439 | train acc: 1.000000\n",
      "epoch 13: 6/125 | train loss: 0.101165 | train acc: 1.000000\n",
      "epoch 13: 7/125 | train loss: 0.430053 | train acc: 0.937500\n",
      "epoch 13: 8/125 | train loss: 0.308759 | train acc: 0.937500\n",
      "epoch 13: 9/125 | train loss: 0.385530 | train acc: 0.843750\n",
      "epoch 13: 10/125 | train loss: 0.160815 | train acc: 0.906250\n",
      "epoch 13: 10/125 | test loss: 0.008621 | test acc: 0.871145\n",
      "epoch 13: 11/125 | train loss: 0.139403 | train acc: 0.968750\n",
      "epoch 13: 12/125 | train loss: 0.237171 | train acc: 0.937500\n",
      "epoch 13: 13/125 | train loss: 0.073668 | train acc: 0.968750\n",
      "epoch 13: 14/125 | train loss: 0.330389 | train acc: 0.875000\n",
      "epoch 13: 15/125 | train loss: 0.201170 | train acc: 0.875000\n",
      "epoch 13: 16/125 | train loss: 0.132040 | train acc: 1.000000\n",
      "epoch 13: 17/125 | train loss: 0.268944 | train acc: 0.906250\n",
      "epoch 13: 18/125 | train loss: 0.188635 | train acc: 0.937500\n",
      "epoch 13: 19/125 | train loss: 0.317820 | train acc: 0.875000\n",
      "epoch 13: 20/125 | train loss: 0.239987 | train acc: 0.875000\n",
      "epoch 13: 20/125 | test loss: 0.012941 | test acc: 0.870044\n",
      "epoch 13: 21/125 | train loss: 0.246759 | train acc: 0.906250\n",
      "epoch 13: 22/125 | train loss: 0.093887 | train acc: 0.968750\n",
      "epoch 13: 23/125 | train loss: 0.157212 | train acc: 0.937500\n",
      "epoch 13: 24/125 | train loss: 0.138963 | train acc: 0.968750\n",
      "epoch 13: 25/125 | train loss: 0.147653 | train acc: 0.968750\n",
      "epoch 13: 26/125 | train loss: 0.215941 | train acc: 0.937500\n",
      "epoch 13: 27/125 | train loss: 0.319655 | train acc: 0.875000\n",
      "epoch 13: 28/125 | train loss: 0.206343 | train acc: 0.906250\n",
      "epoch 13: 29/125 | train loss: 0.430292 | train acc: 0.875000\n",
      "epoch 13: 30/125 | train loss: 0.148751 | train acc: 0.937500\n",
      "epoch 13: 30/125 | test loss: 0.012106 | test acc: 0.874449\n",
      "epoch 13: 31/125 | train loss: 0.228498 | train acc: 0.875000\n",
      "epoch 13: 32/125 | train loss: 0.172589 | train acc: 0.906250\n",
      "epoch 13: 33/125 | train loss: 0.283102 | train acc: 0.906250\n",
      "epoch 13: 34/125 | train loss: 0.393630 | train acc: 0.906250\n",
      "epoch 13: 35/125 | train loss: 0.182729 | train acc: 0.937500\n",
      "epoch 13: 36/125 | train loss: 0.521838 | train acc: 0.875000\n",
      "epoch 13: 37/125 | train loss: 0.360896 | train acc: 0.875000\n",
      "epoch 13: 38/125 | train loss: 0.107662 | train acc: 1.000000\n",
      "epoch 13: 39/125 | train loss: 0.243383 | train acc: 0.937500\n",
      "epoch 13: 40/125 | train loss: 0.276273 | train acc: 0.906250\n",
      "epoch 13: 40/125 | test loss: 0.010448 | test acc: 0.863436\n",
      "epoch 13: 41/125 | train loss: 0.564640 | train acc: 0.812500\n",
      "epoch 13: 42/125 | train loss: 0.404881 | train acc: 0.875000\n",
      "epoch 13: 43/125 | train loss: 0.089765 | train acc: 0.968750\n",
      "epoch 13: 44/125 | train loss: 0.218148 | train acc: 0.875000\n",
      "epoch 13: 45/125 | train loss: 0.317639 | train acc: 0.875000\n",
      "epoch 13: 46/125 | train loss: 0.224029 | train acc: 0.937500\n",
      "epoch 13: 47/125 | train loss: 0.209707 | train acc: 0.968750\n",
      "epoch 13: 48/125 | train loss: 0.220793 | train acc: 0.937500\n",
      "epoch 13: 49/125 | train loss: 0.280245 | train acc: 0.906250\n",
      "epoch 13: 50/125 | train loss: 0.164880 | train acc: 0.937500\n",
      "epoch 13: 50/125 | test loss: 0.011590 | test acc: 0.866740\n",
      "epoch 13: 51/125 | train loss: 0.171183 | train acc: 0.968750\n",
      "epoch 13: 52/125 | train loss: 0.354628 | train acc: 0.843750\n",
      "epoch 13: 53/125 | train loss: 0.092850 | train acc: 0.968750\n",
      "epoch 13: 54/125 | train loss: 0.184722 | train acc: 0.937500\n",
      "epoch 13: 55/125 | train loss: 0.246379 | train acc: 0.906250\n",
      "epoch 13: 56/125 | train loss: 0.115310 | train acc: 1.000000\n",
      "epoch 13: 57/125 | train loss: 0.117213 | train acc: 0.968750\n",
      "epoch 13: 58/125 | train loss: 0.162918 | train acc: 0.937500\n",
      "epoch 13: 59/125 | train loss: 0.095277 | train acc: 1.000000\n",
      "epoch 13: 60/125 | train loss: 0.133630 | train acc: 0.937500\n",
      "epoch 13: 60/125 | test loss: 0.006030 | test acc: 0.867841\n",
      "epoch 13: 61/125 | train loss: 0.178093 | train acc: 0.937500\n",
      "epoch 13: 62/125 | train loss: 0.171937 | train acc: 0.968750\n",
      "epoch 13: 63/125 | train loss: 0.197231 | train acc: 0.968750\n",
      "epoch 13: 64/125 | train loss: 0.212841 | train acc: 0.937500\n",
      "epoch 13: 65/125 | train loss: 0.134008 | train acc: 0.968750\n",
      "epoch 13: 66/125 | train loss: 0.096827 | train acc: 1.000000\n",
      "epoch 13: 67/125 | train loss: 0.362053 | train acc: 0.906250\n",
      "epoch 13: 68/125 | train loss: 0.159234 | train acc: 0.968750\n",
      "epoch 13: 69/125 | train loss: 0.300746 | train acc: 0.843750\n",
      "epoch 13: 70/125 | train loss: 0.329872 | train acc: 0.875000\n",
      "epoch 13: 70/125 | test loss: 0.004956 | test acc: 0.865639\n",
      "epoch 13: 71/125 | train loss: 0.140869 | train acc: 0.937500\n",
      "epoch 13: 72/125 | train loss: 0.377069 | train acc: 0.781250\n",
      "epoch 13: 73/125 | train loss: 0.671729 | train acc: 0.812500\n",
      "epoch 13: 74/125 | train loss: 0.148914 | train acc: 0.937500\n",
      "epoch 13: 75/125 | train loss: 0.181679 | train acc: 0.937500\n",
      "epoch 13: 76/125 | train loss: 0.178634 | train acc: 0.968750\n",
      "epoch 13: 77/125 | train loss: 0.156144 | train acc: 0.937500\n",
      "epoch 13: 78/125 | train loss: 0.174610 | train acc: 0.937500\n",
      "epoch 13: 79/125 | train loss: 0.274601 | train acc: 0.906250\n",
      "epoch 13: 80/125 | train loss: 0.208808 | train acc: 0.937500\n",
      "epoch 13: 80/125 | test loss: 0.006943 | test acc: 0.867841\n",
      "epoch 13: 81/125 | train loss: 0.137486 | train acc: 0.968750\n",
      "epoch 13: 82/125 | train loss: 0.280801 | train acc: 0.906250\n",
      "epoch 13: 83/125 | train loss: 0.240827 | train acc: 0.906250\n",
      "epoch 13: 84/125 | train loss: 0.263241 | train acc: 0.843750\n",
      "epoch 13: 85/125 | train loss: 0.137231 | train acc: 0.937500\n",
      "epoch 13: 86/125 | train loss: 0.205122 | train acc: 0.937500\n",
      "epoch 13: 87/125 | train loss: 0.196695 | train acc: 0.937500\n",
      "epoch 13: 88/125 | train loss: 0.179473 | train acc: 0.937500\n",
      "epoch 13: 89/125 | train loss: 0.216682 | train acc: 0.937500\n",
      "epoch 13: 90/125 | train loss: 0.411820 | train acc: 0.843750\n",
      "epoch 13: 90/125 | test loss: 0.008231 | test acc: 0.871145\n",
      "epoch 13: 91/125 | train loss: 0.289600 | train acc: 0.875000\n",
      "epoch 13: 92/125 | train loss: 0.152814 | train acc: 0.968750\n",
      "epoch 13: 93/125 | train loss: 0.297687 | train acc: 0.875000\n",
      "epoch 13: 94/125 | train loss: 0.159590 | train acc: 0.968750\n",
      "epoch 13: 95/125 | train loss: 0.271899 | train acc: 0.906250\n",
      "epoch 13: 96/125 | train loss: 0.060187 | train acc: 1.000000\n",
      "epoch 13: 97/125 | train loss: 0.479546 | train acc: 0.843750\n",
      "epoch 13: 98/125 | train loss: 0.319429 | train acc: 0.875000\n",
      "epoch 13: 99/125 | train loss: 0.078575 | train acc: 1.000000\n",
      "epoch 13: 100/125 | train loss: 0.335814 | train acc: 0.781250\n",
      "epoch 13: 100/125 | test loss: 0.005724 | test acc: 0.870044\n",
      "epoch 13: 101/125 | train loss: 0.216021 | train acc: 0.875000\n",
      "epoch 13: 102/125 | train loss: 0.220395 | train acc: 0.906250\n",
      "epoch 13: 103/125 | train loss: 0.333985 | train acc: 0.875000\n",
      "epoch 13: 104/125 | train loss: 0.224756 | train acc: 0.906250\n",
      "epoch 13: 105/125 | train loss: 0.365288 | train acc: 0.906250\n",
      "epoch 13: 106/125 | train loss: 0.233947 | train acc: 0.937500\n",
      "epoch 13: 107/125 | train loss: 0.173095 | train acc: 0.937500\n",
      "epoch 13: 108/125 | train loss: 0.387954 | train acc: 0.875000\n",
      "epoch 13: 109/125 | train loss: 0.076218 | train acc: 0.968750\n",
      "epoch 13: 110/125 | train loss: 0.125871 | train acc: 0.937500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 13: 110/125 | test loss: 0.008598 | test acc: 0.873348\n",
      "epoch 13: 111/125 | train loss: 0.137923 | train acc: 0.968750\n",
      "epoch 13: 112/125 | train loss: 0.083145 | train acc: 1.000000\n",
      "epoch 13: 113/125 | train loss: 0.321972 | train acc: 0.906250\n",
      "epoch 13: 114/125 | train loss: 0.306371 | train acc: 0.906250\n",
      "epoch 13: 115/125 | train loss: 0.242096 | train acc: 0.843750\n",
      "epoch 13: 116/125 | train loss: 0.205895 | train acc: 0.968750\n",
      "epoch 13: 117/125 | train loss: 0.219234 | train acc: 0.906250\n",
      "epoch 13: 118/125 | train loss: 0.176260 | train acc: 0.906250\n",
      "epoch 13: 119/125 | train loss: 0.144338 | train acc: 0.937500\n",
      "epoch 13: 120/125 | train loss: 0.181100 | train acc: 0.937500\n",
      "epoch 13: 120/125 | test loss: 0.016403 | test acc: 0.872247\n",
      "epoch 13: 121/125 | train loss: 0.094931 | train acc: 0.968750\n",
      "epoch 13: 122/125 | train loss: 0.220244 | train acc: 0.937500\n",
      "epoch 13: 123/125 | train loss: 0.126453 | train acc: 0.968750\n",
      "epoch 13: 124/125 | train loss: 0.110109 | train acc: 0.937500\n",
      "epoch 13: 125/125 | train loss: 0.432167 | train acc: 0.625000\n",
      "epoch 13 | mean train acc: 0.919000\n",
      "epoch 13 | mean test acc: 0.869493\n",
      "epoch 14: 1/125 | train loss: 0.189342 | train acc: 0.968750\n",
      "epoch 14: 2/125 | train loss: 0.213295 | train acc: 0.875000\n",
      "epoch 14: 3/125 | train loss: 0.225744 | train acc: 0.906250\n",
      "epoch 14: 4/125 | train loss: 0.119746 | train acc: 0.968750\n",
      "epoch 14: 5/125 | train loss: 0.474574 | train acc: 0.843750\n",
      "epoch 14: 6/125 | train loss: 0.109121 | train acc: 0.968750\n",
      "epoch 14: 7/125 | train loss: 0.294676 | train acc: 0.906250\n",
      "epoch 14: 8/125 | train loss: 0.110863 | train acc: 1.000000\n",
      "epoch 14: 9/125 | train loss: 0.341776 | train acc: 0.875000\n",
      "epoch 14: 10/125 | train loss: 0.088055 | train acc: 0.968750\n",
      "epoch 14: 10/125 | test loss: 0.007008 | test acc: 0.867841\n",
      "epoch 14: 11/125 | train loss: 0.171921 | train acc: 0.937500\n",
      "epoch 14: 12/125 | train loss: 0.224965 | train acc: 0.906250\n",
      "epoch 14: 13/125 | train loss: 0.333985 | train acc: 0.906250\n",
      "epoch 14: 14/125 | train loss: 0.377145 | train acc: 0.906250\n",
      "epoch 14: 15/125 | train loss: 0.278887 | train acc: 0.906250\n",
      "epoch 14: 16/125 | train loss: 0.196148 | train acc: 0.937500\n",
      "epoch 14: 17/125 | train loss: 0.073385 | train acc: 1.000000\n",
      "epoch 14: 18/125 | train loss: 0.094157 | train acc: 1.000000\n",
      "epoch 14: 19/125 | train loss: 0.157795 | train acc: 0.937500\n",
      "epoch 14: 20/125 | train loss: 0.069009 | train acc: 1.000000\n",
      "epoch 14: 20/125 | test loss: 0.010286 | test acc: 0.872247\n",
      "epoch 14: 21/125 | train loss: 0.136725 | train acc: 0.968750\n",
      "epoch 14: 22/125 | train loss: 0.168855 | train acc: 0.968750\n",
      "epoch 14: 23/125 | train loss: 0.153509 | train acc: 0.906250\n",
      "epoch 14: 24/125 | train loss: 0.137598 | train acc: 0.937500\n",
      "epoch 14: 25/125 | train loss: 0.177408 | train acc: 0.937500\n",
      "epoch 14: 26/125 | train loss: 0.099737 | train acc: 0.968750\n",
      "epoch 14: 27/125 | train loss: 0.397456 | train acc: 0.968750\n",
      "epoch 14: 28/125 | train loss: 0.362496 | train acc: 0.906250\n",
      "epoch 14: 29/125 | train loss: 0.269072 | train acc: 0.937500\n",
      "epoch 14: 30/125 | train loss: 0.210724 | train acc: 0.937500\n",
      "epoch 14: 30/125 | test loss: 0.012479 | test acc: 0.865639\n",
      "epoch 14: 31/125 | train loss: 0.282040 | train acc: 0.843750\n",
      "epoch 14: 32/125 | train loss: 0.299060 | train acc: 0.906250\n",
      "epoch 14: 33/125 | train loss: 0.096060 | train acc: 0.968750\n",
      "epoch 14: 34/125 | train loss: 0.223905 | train acc: 0.968750\n",
      "epoch 14: 35/125 | train loss: 0.150017 | train acc: 0.968750\n",
      "epoch 14: 36/125 | train loss: 0.078219 | train acc: 1.000000\n",
      "epoch 14: 37/125 | train loss: 0.141232 | train acc: 0.968750\n",
      "epoch 14: 38/125 | train loss: 0.163614 | train acc: 0.937500\n",
      "epoch 14: 39/125 | train loss: 0.101993 | train acc: 0.968750\n",
      "epoch 14: 40/125 | train loss: 0.447188 | train acc: 0.875000\n",
      "epoch 14: 40/125 | test loss: 0.011303 | test acc: 0.865639\n",
      "epoch 14: 41/125 | train loss: 0.306809 | train acc: 0.843750\n",
      "epoch 14: 42/125 | train loss: 0.149590 | train acc: 0.937500\n",
      "epoch 14: 43/125 | train loss: 0.247364 | train acc: 0.875000\n",
      "epoch 14: 44/125 | train loss: 0.360034 | train acc: 0.875000\n",
      "epoch 14: 45/125 | train loss: 0.266809 | train acc: 0.906250\n",
      "epoch 14: 46/125 | train loss: 0.303591 | train acc: 0.843750\n",
      "epoch 14: 47/125 | train loss: 0.396167 | train acc: 0.875000\n",
      "epoch 14: 48/125 | train loss: 0.090371 | train acc: 0.968750\n",
      "epoch 14: 49/125 | train loss: 0.433875 | train acc: 0.750000\n",
      "epoch 14: 50/125 | train loss: 0.188433 | train acc: 0.906250\n",
      "epoch 14: 50/125 | test loss: 0.007784 | test acc: 0.863436\n",
      "epoch 14: 51/125 | train loss: 0.396418 | train acc: 0.906250\n",
      "epoch 14: 52/125 | train loss: 0.185885 | train acc: 0.906250\n",
      "epoch 14: 53/125 | train loss: 0.319149 | train acc: 0.843750\n",
      "epoch 14: 54/125 | train loss: 0.290466 | train acc: 0.875000\n",
      "epoch 14: 55/125 | train loss: 0.350743 | train acc: 0.843750\n",
      "epoch 14: 56/125 | train loss: 0.832837 | train acc: 0.750000\n",
      "epoch 14: 57/125 | train loss: 0.175072 | train acc: 0.968750\n",
      "epoch 14: 58/125 | train loss: 0.137526 | train acc: 0.937500\n",
      "epoch 14: 59/125 | train loss: 0.280653 | train acc: 0.875000\n",
      "epoch 14: 60/125 | train loss: 0.255164 | train acc: 0.875000\n",
      "epoch 14: 60/125 | test loss: 0.014871 | test acc: 0.871145\n",
      "epoch 14: 61/125 | train loss: 0.264308 | train acc: 0.968750\n",
      "epoch 14: 62/125 | train loss: 0.196697 | train acc: 0.906250\n",
      "epoch 14: 63/125 | train loss: 0.137329 | train acc: 0.968750\n",
      "epoch 14: 64/125 | train loss: 0.307003 | train acc: 0.875000\n",
      "epoch 14: 65/125 | train loss: 0.256993 | train acc: 0.937500\n",
      "epoch 14: 66/125 | train loss: 0.129024 | train acc: 0.968750\n",
      "epoch 14: 67/125 | train loss: 0.535950 | train acc: 0.812500\n",
      "epoch 14: 68/125 | train loss: 0.071409 | train acc: 1.000000\n",
      "epoch 14: 69/125 | train loss: 0.345845 | train acc: 0.906250\n",
      "epoch 14: 70/125 | train loss: 0.299386 | train acc: 0.906250\n",
      "epoch 14: 70/125 | test loss: 0.015971 | test acc: 0.873348\n",
      "epoch 14: 71/125 | train loss: 0.184804 | train acc: 0.937500\n",
      "epoch 14: 72/125 | train loss: 0.204280 | train acc: 0.968750\n",
      "epoch 14: 73/125 | train loss: 0.267429 | train acc: 0.875000\n",
      "epoch 14: 74/125 | train loss: 0.216433 | train acc: 0.937500\n",
      "epoch 14: 75/125 | train loss: 0.233234 | train acc: 0.875000\n",
      "epoch 14: 76/125 | train loss: 0.161862 | train acc: 0.968750\n",
      "epoch 14: 77/125 | train loss: 0.263321 | train acc: 0.875000\n",
      "epoch 14: 78/125 | train loss: 0.235052 | train acc: 0.937500\n",
      "epoch 14: 79/125 | train loss: 0.108400 | train acc: 0.968750\n",
      "epoch 14: 80/125 | train loss: 0.287174 | train acc: 0.843750\n",
      "epoch 14: 80/125 | test loss: 0.007750 | test acc: 0.874449\n",
      "epoch 14: 81/125 | train loss: 0.602294 | train acc: 0.750000\n",
      "epoch 14: 82/125 | train loss: 0.210392 | train acc: 0.937500\n",
      "epoch 14: 83/125 | train loss: 0.311171 | train acc: 0.843750\n",
      "epoch 14: 84/125 | train loss: 0.077092 | train acc: 0.968750\n",
      "epoch 14: 85/125 | train loss: 0.151080 | train acc: 0.937500\n",
      "epoch 14: 86/125 | train loss: 0.110867 | train acc: 0.968750\n",
      "epoch 14: 87/125 | train loss: 0.069510 | train acc: 1.000000\n",
      "epoch 14: 88/125 | train loss: 0.077900 | train acc: 0.968750\n",
      "epoch 14: 89/125 | train loss: 0.485937 | train acc: 0.843750\n",
      "epoch 14: 90/125 | train loss: 0.234795 | train acc: 0.906250\n",
      "epoch 14: 90/125 | test loss: 0.007281 | test acc: 0.867841\n",
      "epoch 14: 91/125 | train loss: 0.113579 | train acc: 0.968750\n",
      "epoch 14: 92/125 | train loss: 0.105926 | train acc: 0.968750\n",
      "epoch 14: 93/125 | train loss: 0.417475 | train acc: 0.875000\n",
      "epoch 14: 94/125 | train loss: 0.140686 | train acc: 1.000000\n",
      "epoch 14: 95/125 | train loss: 0.412440 | train acc: 0.875000\n",
      "epoch 14: 96/125 | train loss: 0.133335 | train acc: 0.968750\n",
      "epoch 14: 97/125 | train loss: 0.192061 | train acc: 0.875000\n",
      "epoch 14: 98/125 | train loss: 0.117375 | train acc: 0.968750\n",
      "epoch 14: 99/125 | train loss: 0.329470 | train acc: 0.906250\n",
      "epoch 14: 100/125 | train loss: 0.172216 | train acc: 0.875000\n",
      "epoch 14: 100/125 | test loss: 0.008807 | test acc: 0.873348\n",
      "epoch 14: 101/125 | train loss: 0.193775 | train acc: 0.968750\n",
      "epoch 14: 102/125 | train loss: 0.401792 | train acc: 0.875000\n",
      "epoch 14: 103/125 | train loss: 0.279244 | train acc: 0.875000\n",
      "epoch 14: 104/125 | train loss: 0.257642 | train acc: 0.937500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 14: 105/125 | train loss: 0.285196 | train acc: 0.875000\n",
      "epoch 14: 106/125 | train loss: 0.168108 | train acc: 0.906250\n",
      "epoch 14: 107/125 | train loss: 0.462765 | train acc: 0.875000\n",
      "epoch 14: 108/125 | train loss: 0.124348 | train acc: 0.968750\n",
      "epoch 14: 109/125 | train loss: 0.108646 | train acc: 0.968750\n",
      "epoch 14: 110/125 | train loss: 0.254882 | train acc: 0.875000\n",
      "epoch 14: 110/125 | test loss: 0.008456 | test acc: 0.867841\n",
      "epoch 14: 111/125 | train loss: 0.149001 | train acc: 0.937500\n",
      "epoch 14: 112/125 | train loss: 0.266612 | train acc: 0.937500\n",
      "epoch 14: 113/125 | train loss: 0.141330 | train acc: 0.937500\n",
      "epoch 14: 114/125 | train loss: 0.188786 | train acc: 0.968750\n",
      "epoch 14: 115/125 | train loss: 0.086859 | train acc: 0.968750\n",
      "epoch 14: 116/125 | train loss: 0.322495 | train acc: 0.906250\n",
      "epoch 14: 117/125 | train loss: 0.357744 | train acc: 0.937500\n",
      "epoch 14: 118/125 | train loss: 0.194554 | train acc: 0.906250\n",
      "epoch 14: 119/125 | train loss: 0.143848 | train acc: 0.968750\n",
      "epoch 14: 120/125 | train loss: 0.387074 | train acc: 0.875000\n",
      "epoch 14: 120/125 | test loss: 0.011854 | test acc: 0.868943\n",
      "epoch 14: 121/125 | train loss: 0.323505 | train acc: 0.843750\n",
      "epoch 14: 122/125 | train loss: 0.213509 | train acc: 0.937500\n",
      "epoch 14: 123/125 | train loss: 0.092717 | train acc: 1.000000\n",
      "epoch 14: 124/125 | train loss: 0.163628 | train acc: 0.937500\n",
      "epoch 14: 125/125 | train loss: 0.191658 | train acc: 0.687500\n",
      "epoch 14 | mean train acc: 0.918750\n",
      "epoch 14 | mean test acc: 0.869310\n",
      "epoch 15: 1/125 | train loss: 0.538830 | train acc: 0.843750\n",
      "epoch 15: 2/125 | train loss: 0.230491 | train acc: 0.937500\n",
      "epoch 15: 3/125 | train loss: 0.188163 | train acc: 0.906250\n",
      "epoch 15: 4/125 | train loss: 0.121291 | train acc: 0.968750\n",
      "epoch 15: 5/125 | train loss: 0.184006 | train acc: 0.875000\n",
      "epoch 15: 6/125 | train loss: 0.311621 | train acc: 0.906250\n",
      "epoch 15: 7/125 | train loss: 0.079610 | train acc: 1.000000\n",
      "epoch 15: 8/125 | train loss: 0.166457 | train acc: 1.000000\n",
      "epoch 15: 9/125 | train loss: 0.111567 | train acc: 0.968750\n",
      "epoch 15: 10/125 | train loss: 0.217403 | train acc: 0.875000\n",
      "epoch 15: 10/125 | test loss: 0.015491 | test acc: 0.872247\n",
      "epoch 15: 11/125 | train loss: 0.253638 | train acc: 0.843750\n",
      "epoch 15: 12/125 | train loss: 0.263607 | train acc: 0.937500\n",
      "epoch 15: 13/125 | train loss: 0.166211 | train acc: 0.968750\n",
      "epoch 15: 14/125 | train loss: 0.099460 | train acc: 0.968750\n",
      "epoch 15: 15/125 | train loss: 0.398829 | train acc: 0.843750\n",
      "epoch 15: 16/125 | train loss: 0.119092 | train acc: 1.000000\n",
      "epoch 15: 17/125 | train loss: 0.265184 | train acc: 0.906250\n",
      "epoch 15: 18/125 | train loss: 0.290349 | train acc: 0.906250\n",
      "epoch 15: 19/125 | train loss: 0.076016 | train acc: 1.000000\n",
      "epoch 15: 20/125 | train loss: 0.114577 | train acc: 0.968750\n",
      "epoch 15: 20/125 | test loss: 0.011987 | test acc: 0.870044\n",
      "epoch 15: 21/125 | train loss: 0.231725 | train acc: 0.937500\n",
      "epoch 15: 22/125 | train loss: 0.108032 | train acc: 1.000000\n",
      "epoch 15: 23/125 | train loss: 0.292634 | train acc: 0.906250\n",
      "epoch 15: 24/125 | train loss: 0.323946 | train acc: 0.843750\n",
      "epoch 15: 25/125 | train loss: 0.180830 | train acc: 0.937500\n",
      "epoch 15: 26/125 | train loss: 0.320400 | train acc: 0.875000\n",
      "epoch 15: 27/125 | train loss: 0.253861 | train acc: 0.906250\n",
      "epoch 15: 28/125 | train loss: 0.183327 | train acc: 0.968750\n",
      "epoch 15: 29/125 | train loss: 0.140523 | train acc: 0.937500\n",
      "epoch 15: 30/125 | train loss: 0.163229 | train acc: 0.968750\n",
      "epoch 15: 30/125 | test loss: 0.011007 | test acc: 0.865639\n",
      "epoch 15: 31/125 | train loss: 0.293343 | train acc: 0.906250\n",
      "epoch 15: 32/125 | train loss: 0.135161 | train acc: 0.968750\n",
      "epoch 15: 33/125 | train loss: 0.274698 | train acc: 0.843750\n",
      "epoch 15: 34/125 | train loss: 0.288222 | train acc: 0.875000\n",
      "epoch 15: 35/125 | train loss: 0.192832 | train acc: 0.937500\n",
      "epoch 15: 36/125 | train loss: 0.193823 | train acc: 0.968750\n",
      "epoch 15: 37/125 | train loss: 0.129806 | train acc: 0.968750\n",
      "epoch 15: 38/125 | train loss: 0.112707 | train acc: 0.968750\n",
      "epoch 15: 39/125 | train loss: 0.070738 | train acc: 1.000000\n",
      "epoch 15: 40/125 | train loss: 0.089002 | train acc: 1.000000\n",
      "epoch 15: 40/125 | test loss: 0.006159 | test acc: 0.868943\n",
      "epoch 15: 41/125 | train loss: 0.421526 | train acc: 0.812500\n",
      "epoch 15: 42/125 | train loss: 0.134970 | train acc: 1.000000\n",
      "epoch 15: 43/125 | train loss: 0.119991 | train acc: 0.968750\n",
      "epoch 15: 44/125 | train loss: 0.139869 | train acc: 0.968750\n",
      "epoch 15: 45/125 | train loss: 0.339041 | train acc: 0.875000\n",
      "epoch 15: 46/125 | train loss: 0.297974 | train acc: 0.906250\n",
      "epoch 15: 47/125 | train loss: 0.136610 | train acc: 0.968750\n",
      "epoch 15: 48/125 | train loss: 0.491670 | train acc: 0.781250\n",
      "epoch 15: 49/125 | train loss: 0.443116 | train acc: 0.843750\n",
      "epoch 15: 50/125 | train loss: 0.201650 | train acc: 0.906250\n",
      "epoch 15: 50/125 | test loss: 0.005425 | test acc: 0.871145\n",
      "epoch 15: 51/125 | train loss: 0.315513 | train acc: 0.875000\n",
      "epoch 15: 52/125 | train loss: 0.071498 | train acc: 0.968750\n",
      "epoch 15: 53/125 | train loss: 0.133871 | train acc: 0.937500\n",
      "epoch 15: 54/125 | train loss: 0.099899 | train acc: 1.000000\n",
      "epoch 15: 55/125 | train loss: 0.183548 | train acc: 0.937500\n",
      "epoch 15: 56/125 | train loss: 0.246159 | train acc: 0.875000\n",
      "epoch 15: 57/125 | train loss: 0.252467 | train acc: 0.937500\n",
      "epoch 15: 58/125 | train loss: 0.181634 | train acc: 0.937500\n",
      "epoch 15: 59/125 | train loss: 0.196377 | train acc: 0.968750\n",
      "epoch 15: 60/125 | train loss: 0.206953 | train acc: 0.937500\n",
      "epoch 15: 60/125 | test loss: 0.013539 | test acc: 0.872247\n",
      "epoch 15: 61/125 | train loss: 0.146985 | train acc: 0.968750\n",
      "epoch 15: 62/125 | train loss: 0.157351 | train acc: 0.906250\n",
      "epoch 15: 63/125 | train loss: 0.128952 | train acc: 0.937500\n",
      "epoch 15: 64/125 | train loss: 0.339930 | train acc: 0.843750\n",
      "epoch 15: 65/125 | train loss: 0.143387 | train acc: 0.906250\n",
      "epoch 15: 66/125 | train loss: 0.079702 | train acc: 0.968750\n",
      "epoch 15: 67/125 | train loss: 0.184008 | train acc: 0.937500\n",
      "epoch 15: 68/125 | train loss: 0.443698 | train acc: 0.875000\n",
      "epoch 15: 69/125 | train loss: 0.487930 | train acc: 0.875000\n",
      "epoch 15: 70/125 | train loss: 0.380059 | train acc: 0.781250\n",
      "epoch 15: 70/125 | test loss: 0.010678 | test acc: 0.875551\n",
      "epoch 15: 71/125 | train loss: 0.226999 | train acc: 0.875000\n",
      "epoch 15: 72/125 | train loss: 0.197645 | train acc: 0.937500\n",
      "epoch 15: 73/125 | train loss: 0.147790 | train acc: 0.968750\n",
      "epoch 15: 74/125 | train loss: 0.069762 | train acc: 1.000000\n",
      "epoch 15: 75/125 | train loss: 0.505932 | train acc: 0.812500\n",
      "epoch 15: 76/125 | train loss: 0.204098 | train acc: 0.937500\n",
      "epoch 15: 77/125 | train loss: 0.160990 | train acc: 0.968750\n",
      "epoch 15: 78/125 | train loss: 0.194036 | train acc: 0.906250\n",
      "epoch 15: 79/125 | train loss: 0.247915 | train acc: 0.937500\n",
      "epoch 15: 80/125 | train loss: 0.195683 | train acc: 0.906250\n",
      "epoch 15: 80/125 | test loss: 0.014253 | test acc: 0.867841\n",
      "epoch 15: 81/125 | train loss: 0.527992 | train acc: 0.843750\n",
      "epoch 15: 82/125 | train loss: 0.167699 | train acc: 0.968750\n",
      "epoch 15: 83/125 | train loss: 0.133124 | train acc: 0.968750\n",
      "epoch 15: 84/125 | train loss: 0.148893 | train acc: 0.968750\n",
      "epoch 15: 85/125 | train loss: 0.233628 | train acc: 0.906250\n",
      "epoch 15: 86/125 | train loss: 0.286961 | train acc: 0.875000\n",
      "epoch 15: 87/125 | train loss: 0.204232 | train acc: 0.875000\n",
      "epoch 15: 88/125 | train loss: 0.279972 | train acc: 0.875000\n",
      "epoch 15: 89/125 | train loss: 0.214257 | train acc: 0.906250\n",
      "epoch 15: 90/125 | train loss: 0.117350 | train acc: 1.000000\n",
      "epoch 15: 90/125 | test loss: 0.010627 | test acc: 0.871145\n",
      "epoch 15: 91/125 | train loss: 0.629651 | train acc: 0.812500\n",
      "epoch 15: 92/125 | train loss: 0.128408 | train acc: 1.000000\n",
      "epoch 15: 93/125 | train loss: 0.347651 | train acc: 0.906250\n",
      "epoch 15: 94/125 | train loss: 0.348726 | train acc: 0.875000\n",
      "epoch 15: 95/125 | train loss: 0.316140 | train acc: 0.906250\n",
      "epoch 15: 96/125 | train loss: 0.143735 | train acc: 0.937500\n",
      "epoch 15: 97/125 | train loss: 0.227087 | train acc: 0.968750\n",
      "epoch 15: 98/125 | train loss: 0.264833 | train acc: 0.906250\n",
      "epoch 15: 99/125 | train loss: 0.099328 | train acc: 0.937500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 15: 100/125 | train loss: 0.253271 | train acc: 0.906250\n",
      "epoch 15: 100/125 | test loss: 0.009195 | test acc: 0.863436\n",
      "epoch 15: 101/125 | train loss: 0.288831 | train acc: 0.906250\n",
      "epoch 15: 102/125 | train loss: 0.109381 | train acc: 0.937500\n",
      "epoch 15: 103/125 | train loss: 0.180588 | train acc: 0.968750\n",
      "epoch 15: 104/125 | train loss: 0.150044 | train acc: 1.000000\n",
      "epoch 15: 105/125 | train loss: 0.375429 | train acc: 0.812500\n",
      "epoch 15: 106/125 | train loss: 0.287734 | train acc: 0.906250\n",
      "epoch 15: 107/125 | train loss: 0.204048 | train acc: 0.937500\n",
      "epoch 15: 108/125 | train loss: 0.613562 | train acc: 0.812500\n",
      "epoch 15: 109/125 | train loss: 0.361727 | train acc: 0.812500\n",
      "epoch 15: 110/125 | train loss: 0.182752 | train acc: 0.937500\n",
      "epoch 15: 110/125 | test loss: 0.006696 | test acc: 0.867841\n",
      "epoch 15: 111/125 | train loss: 0.254331 | train acc: 0.875000\n",
      "epoch 15: 112/125 | train loss: 0.239904 | train acc: 0.937500\n",
      "epoch 15: 113/125 | train loss: 0.152545 | train acc: 0.937500\n",
      "epoch 15: 114/125 | train loss: 0.245573 | train acc: 0.906250\n",
      "epoch 15: 115/125 | train loss: 0.133962 | train acc: 0.968750\n",
      "epoch 15: 116/125 | train loss: 0.293249 | train acc: 0.812500\n",
      "epoch 15: 117/125 | train loss: 0.279225 | train acc: 0.906250\n",
      "epoch 15: 118/125 | train loss: 0.255834 | train acc: 0.906250\n",
      "epoch 15: 119/125 | train loss: 0.145814 | train acc: 0.937500\n",
      "epoch 15: 120/125 | train loss: 0.186266 | train acc: 0.906250\n",
      "epoch 15: 120/125 | test loss: 0.009094 | test acc: 0.862335\n",
      "epoch 15: 121/125 | train loss: 0.191588 | train acc: 0.906250\n",
      "epoch 15: 122/125 | train loss: 0.187620 | train acc: 0.937500\n",
      "epoch 15: 123/125 | train loss: 0.435032 | train acc: 0.906250\n",
      "epoch 15: 124/125 | train loss: 0.229979 | train acc: 0.937500\n",
      "epoch 15: 125/125 | train loss: 0.905332 | train acc: 0.625000\n",
      "epoch 15 | mean train acc: 0.918250\n",
      "epoch 15 | mean test acc: 0.869035\n",
      "epoch 16: 1/125 | train loss: 0.501252 | train acc: 0.843750\n",
      "epoch 16: 2/125 | train loss: 0.211469 | train acc: 0.968750\n",
      "epoch 16: 3/125 | train loss: 0.232423 | train acc: 0.906250\n",
      "epoch 16: 4/125 | train loss: 0.174349 | train acc: 0.937500\n",
      "epoch 16: 5/125 | train loss: 0.093641 | train acc: 1.000000\n",
      "epoch 16: 6/125 | train loss: 0.616330 | train acc: 0.750000\n",
      "epoch 16: 7/125 | train loss: 0.059021 | train acc: 1.000000\n",
      "epoch 16: 8/125 | train loss: 0.149471 | train acc: 0.968750\n",
      "epoch 16: 9/125 | train loss: 0.216307 | train acc: 0.937500\n",
      "epoch 16: 10/125 | train loss: 0.059612 | train acc: 1.000000\n",
      "epoch 16: 10/125 | test loss: 0.015202 | test acc: 0.867841\n",
      "epoch 16: 11/125 | train loss: 0.229675 | train acc: 0.906250\n",
      "epoch 16: 12/125 | train loss: 0.111757 | train acc: 0.968750\n",
      "epoch 16: 13/125 | train loss: 0.334388 | train acc: 0.906250\n",
      "epoch 16: 14/125 | train loss: 0.118658 | train acc: 0.968750\n",
      "epoch 16: 15/125 | train loss: 0.132264 | train acc: 1.000000\n",
      "epoch 16: 16/125 | train loss: 0.180800 | train acc: 0.937500\n",
      "epoch 16: 17/125 | train loss: 0.204989 | train acc: 0.937500\n",
      "epoch 16: 18/125 | train loss: 0.159875 | train acc: 0.937500\n",
      "epoch 16: 19/125 | train loss: 0.173170 | train acc: 1.000000\n",
      "epoch 16: 20/125 | train loss: 0.574884 | train acc: 0.875000\n",
      "epoch 16: 20/125 | test loss: 0.021388 | test acc: 0.868943\n",
      "epoch 16: 21/125 | train loss: 0.229270 | train acc: 0.906250\n",
      "epoch 16: 22/125 | train loss: 0.191321 | train acc: 0.875000\n",
      "epoch 16: 23/125 | train loss: 0.155904 | train acc: 0.906250\n",
      "epoch 16: 24/125 | train loss: 0.067629 | train acc: 0.968750\n",
      "epoch 16: 25/125 | train loss: 0.458132 | train acc: 0.906250\n",
      "epoch 16: 26/125 | train loss: 0.293395 | train acc: 0.875000\n",
      "epoch 16: 27/125 | train loss: 0.452483 | train acc: 0.812500\n",
      "epoch 16: 28/125 | train loss: 0.248361 | train acc: 0.906250\n",
      "epoch 16: 29/125 | train loss: 0.096850 | train acc: 0.937500\n",
      "epoch 16: 30/125 | train loss: 0.570899 | train acc: 0.812500\n",
      "epoch 16: 30/125 | test loss: 0.015825 | test acc: 0.872247\n",
      "epoch 16: 31/125 | train loss: 0.225802 | train acc: 0.937500\n",
      "epoch 16: 32/125 | train loss: 0.202127 | train acc: 0.906250\n",
      "epoch 16: 33/125 | train loss: 0.175466 | train acc: 0.937500\n",
      "epoch 16: 34/125 | train loss: 0.101824 | train acc: 1.000000\n",
      "epoch 16: 35/125 | train loss: 0.070759 | train acc: 1.000000\n",
      "epoch 16: 36/125 | train loss: 0.125154 | train acc: 0.937500\n",
      "epoch 16: 37/125 | train loss: 0.117772 | train acc: 0.968750\n",
      "epoch 16: 38/125 | train loss: 0.132447 | train acc: 0.968750\n",
      "epoch 16: 39/125 | train loss: 0.316945 | train acc: 0.843750\n",
      "epoch 16: 40/125 | train loss: 0.229750 | train acc: 0.875000\n",
      "epoch 16: 40/125 | test loss: 0.011130 | test acc: 0.866740\n",
      "epoch 16: 41/125 | train loss: 0.438001 | train acc: 0.906250\n",
      "epoch 16: 42/125 | train loss: 0.166163 | train acc: 0.937500\n",
      "epoch 16: 43/125 | train loss: 0.214562 | train acc: 0.906250\n",
      "epoch 16: 44/125 | train loss: 0.240660 | train acc: 0.937500\n",
      "epoch 16: 45/125 | train loss: 0.310775 | train acc: 0.906250\n",
      "epoch 16: 46/125 | train loss: 0.157430 | train acc: 0.937500\n",
      "epoch 16: 47/125 | train loss: 0.243448 | train acc: 0.906250\n",
      "epoch 16: 48/125 | train loss: 0.134960 | train acc: 0.968750\n",
      "epoch 16: 49/125 | train loss: 0.128731 | train acc: 0.968750\n",
      "epoch 16: 50/125 | train loss: 0.108216 | train acc: 0.937500\n",
      "epoch 16: 50/125 | test loss: 0.011188 | test acc: 0.873348\n",
      "epoch 16: 51/125 | train loss: 0.399401 | train acc: 0.843750\n",
      "epoch 16: 52/125 | train loss: 0.273488 | train acc: 0.937500\n",
      "epoch 16: 53/125 | train loss: 0.128939 | train acc: 0.968750\n",
      "epoch 16: 54/125 | train loss: 0.240957 | train acc: 0.906250\n",
      "epoch 16: 55/125 | train loss: 0.176087 | train acc: 0.937500\n",
      "epoch 16: 56/125 | train loss: 0.131387 | train acc: 0.968750\n",
      "epoch 16: 57/125 | train loss: 0.500602 | train acc: 0.812500\n",
      "epoch 16: 58/125 | train loss: 0.417694 | train acc: 0.812500\n",
      "epoch 16: 59/125 | train loss: 0.196188 | train acc: 0.937500\n",
      "epoch 16: 60/125 | train loss: 0.097791 | train acc: 0.968750\n",
      "epoch 16: 60/125 | test loss: 0.007666 | test acc: 0.868943\n",
      "epoch 16: 61/125 | train loss: 0.150663 | train acc: 0.968750\n",
      "epoch 16: 62/125 | train loss: 0.246128 | train acc: 0.937500\n",
      "epoch 16: 63/125 | train loss: 0.259611 | train acc: 0.906250\n",
      "epoch 16: 64/125 | train loss: 0.267261 | train acc: 0.937500\n",
      "epoch 16: 65/125 | train loss: 0.444320 | train acc: 0.812500\n",
      "epoch 16: 66/125 | train loss: 0.291224 | train acc: 0.968750\n",
      "epoch 16: 67/125 | train loss: 0.120383 | train acc: 0.968750\n",
      "epoch 16: 68/125 | train loss: 0.167662 | train acc: 0.968750\n",
      "epoch 16: 69/125 | train loss: 0.163913 | train acc: 0.937500\n",
      "epoch 16: 70/125 | train loss: 0.118550 | train acc: 0.968750\n",
      "epoch 16: 70/125 | test loss: 0.007491 | test acc: 0.870044\n",
      "epoch 16: 71/125 | train loss: 0.212458 | train acc: 0.906250\n",
      "epoch 16: 72/125 | train loss: 0.124646 | train acc: 0.937500\n",
      "epoch 16: 73/125 | train loss: 0.273888 | train acc: 0.906250\n",
      "epoch 16: 74/125 | train loss: 0.265807 | train acc: 0.906250\n",
      "epoch 16: 75/125 | train loss: 0.374549 | train acc: 0.875000\n",
      "epoch 16: 76/125 | train loss: 0.141837 | train acc: 1.000000\n",
      "epoch 16: 77/125 | train loss: 0.254629 | train acc: 0.875000\n",
      "epoch 16: 78/125 | train loss: 0.174959 | train acc: 0.968750\n",
      "epoch 16: 79/125 | train loss: 0.312475 | train acc: 0.906250\n",
      "epoch 16: 80/125 | train loss: 0.053625 | train acc: 1.000000\n",
      "epoch 16: 80/125 | test loss: 0.016593 | test acc: 0.866740\n",
      "epoch 16: 81/125 | train loss: 0.104077 | train acc: 1.000000\n",
      "epoch 16: 82/125 | train loss: 0.263379 | train acc: 0.937500\n",
      "epoch 16: 83/125 | train loss: 0.055636 | train acc: 1.000000\n",
      "epoch 16: 84/125 | train loss: 0.236703 | train acc: 0.937500\n",
      "epoch 16: 85/125 | train loss: 0.186025 | train acc: 0.968750\n",
      "epoch 16: 86/125 | train loss: 0.413607 | train acc: 0.781250\n",
      "epoch 16: 87/125 | train loss: 0.205873 | train acc: 0.937500\n",
      "epoch 16: 88/125 | train loss: 0.207663 | train acc: 0.906250\n",
      "epoch 16: 89/125 | train loss: 0.110995 | train acc: 0.968750\n",
      "epoch 16: 90/125 | train loss: 0.141006 | train acc: 0.937500\n",
      "epoch 16: 90/125 | test loss: 0.005348 | test acc: 0.867841\n",
      "epoch 16: 91/125 | train loss: 0.180923 | train acc: 0.937500\n",
      "epoch 16: 92/125 | train loss: 0.066808 | train acc: 1.000000\n",
      "epoch 16: 93/125 | train loss: 0.230538 | train acc: 0.937500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 16: 94/125 | train loss: 0.243278 | train acc: 0.968750\n",
      "epoch 16: 95/125 | train loss: 0.224547 | train acc: 0.875000\n",
      "epoch 16: 96/125 | train loss: 0.099645 | train acc: 0.968750\n",
      "epoch 16: 97/125 | train loss: 0.090756 | train acc: 0.968750\n",
      "epoch 16: 98/125 | train loss: 0.074739 | train acc: 1.000000\n",
      "epoch 16: 99/125 | train loss: 0.141339 | train acc: 1.000000\n",
      "epoch 16: 100/125 | train loss: 0.265886 | train acc: 0.906250\n",
      "epoch 16: 100/125 | test loss: 0.007120 | test acc: 0.865639\n",
      "epoch 16: 101/125 | train loss: 0.178677 | train acc: 0.937500\n",
      "epoch 16: 102/125 | train loss: 0.220614 | train acc: 0.875000\n",
      "epoch 16: 103/125 | train loss: 0.104607 | train acc: 0.968750\n",
      "epoch 16: 104/125 | train loss: 0.485740 | train acc: 0.812500\n",
      "epoch 16: 105/125 | train loss: 0.281542 | train acc: 0.875000\n",
      "epoch 16: 106/125 | train loss: 0.100592 | train acc: 0.968750\n",
      "epoch 16: 107/125 | train loss: 0.319211 | train acc: 0.906250\n",
      "epoch 16: 108/125 | train loss: 0.343199 | train acc: 0.906250\n",
      "epoch 16: 109/125 | train loss: 0.312603 | train acc: 0.875000\n",
      "epoch 16: 110/125 | train loss: 0.405596 | train acc: 0.875000\n",
      "epoch 16: 110/125 | test loss: 0.004426 | test acc: 0.868943\n",
      "epoch 16: 111/125 | train loss: 0.077045 | train acc: 1.000000\n",
      "epoch 16: 112/125 | train loss: 0.211189 | train acc: 0.937500\n",
      "epoch 16: 113/125 | train loss: 0.185980 | train acc: 0.968750\n",
      "epoch 16: 114/125 | train loss: 0.143745 | train acc: 0.968750\n",
      "epoch 16: 115/125 | train loss: 0.388349 | train acc: 0.875000\n",
      "epoch 16: 116/125 | train loss: 0.111569 | train acc: 0.968750\n",
      "epoch 16: 117/125 | train loss: 0.266731 | train acc: 0.906250\n",
      "epoch 16: 118/125 | train loss: 0.385919 | train acc: 0.875000\n",
      "epoch 16: 119/125 | train loss: 0.294611 | train acc: 0.937500\n",
      "epoch 16: 120/125 | train loss: 0.056560 | train acc: 1.000000\n",
      "epoch 16: 120/125 | test loss: 0.007278 | test acc: 0.872247\n",
      "epoch 16: 121/125 | train loss: 0.185280 | train acc: 0.937500\n",
      "epoch 16: 122/125 | train loss: 0.344074 | train acc: 0.875000\n",
      "epoch 16: 123/125 | train loss: 0.286720 | train acc: 0.906250\n",
      "epoch 16: 124/125 | train loss: 0.170307 | train acc: 0.937500\n",
      "epoch 16: 125/125 | train loss: 0.134932 | train acc: 0.687500\n",
      "epoch 16 | mean train acc: 0.926500\n",
      "epoch 16 | mean test acc: 0.869126\n",
      "epoch 17: 1/125 | train loss: 0.107073 | train acc: 0.968750\n",
      "epoch 17: 2/125 | train loss: 0.150278 | train acc: 0.968750\n",
      "epoch 17: 3/125 | train loss: 0.241968 | train acc: 0.875000\n",
      "epoch 17: 4/125 | train loss: 0.087194 | train acc: 0.968750\n",
      "epoch 17: 5/125 | train loss: 0.171066 | train acc: 0.968750\n",
      "epoch 17: 6/125 | train loss: 0.097089 | train acc: 1.000000\n",
      "epoch 17: 7/125 | train loss: 0.142655 | train acc: 0.906250\n",
      "epoch 17: 8/125 | train loss: 0.295688 | train acc: 0.906250\n",
      "epoch 17: 9/125 | train loss: 0.262653 | train acc: 0.937500\n",
      "epoch 17: 10/125 | train loss: 0.089124 | train acc: 0.968750\n",
      "epoch 17: 10/125 | test loss: 0.008336 | test acc: 0.867841\n",
      "epoch 17: 11/125 | train loss: 0.169526 | train acc: 0.937500\n",
      "epoch 17: 12/125 | train loss: 0.449042 | train acc: 0.843750\n",
      "epoch 17: 13/125 | train loss: 0.360824 | train acc: 0.843750\n",
      "epoch 17: 14/125 | train loss: 0.355584 | train acc: 0.875000\n",
      "epoch 17: 15/125 | train loss: 0.382799 | train acc: 0.906250\n",
      "epoch 17: 16/125 | train loss: 0.275387 | train acc: 0.906250\n",
      "epoch 17: 17/125 | train loss: 0.158257 | train acc: 0.906250\n",
      "epoch 17: 18/125 | train loss: 0.329186 | train acc: 0.875000\n",
      "epoch 17: 19/125 | train loss: 0.198010 | train acc: 0.968750\n",
      "epoch 17: 20/125 | train loss: 0.200406 | train acc: 0.875000\n",
      "epoch 17: 20/125 | test loss: 0.006507 | test acc: 0.875551\n",
      "epoch 17: 21/125 | train loss: 0.154782 | train acc: 0.937500\n",
      "epoch 17: 22/125 | train loss: 0.175959 | train acc: 0.937500\n",
      "epoch 17: 23/125 | train loss: 0.386054 | train acc: 0.843750\n",
      "epoch 17: 24/125 | train loss: 0.088802 | train acc: 1.000000\n",
      "epoch 17: 25/125 | train loss: 0.255306 | train acc: 0.906250\n",
      "epoch 17: 26/125 | train loss: 0.438741 | train acc: 0.843750\n",
      "epoch 17: 27/125 | train loss: 0.180830 | train acc: 0.968750\n",
      "epoch 17: 28/125 | train loss: 0.259960 | train acc: 0.875000\n",
      "epoch 17: 29/125 | train loss: 0.141448 | train acc: 0.937500\n",
      "epoch 17: 30/125 | train loss: 0.130677 | train acc: 0.968750\n",
      "epoch 17: 30/125 | test loss: 0.008007 | test acc: 0.860132\n",
      "epoch 17: 31/125 | train loss: 0.137518 | train acc: 0.968750\n",
      "epoch 17: 32/125 | train loss: 0.351018 | train acc: 0.875000\n",
      "epoch 17: 33/125 | train loss: 0.125956 | train acc: 0.937500\n",
      "epoch 17: 34/125 | train loss: 0.384989 | train acc: 0.812500\n",
      "epoch 17: 35/125 | train loss: 0.423874 | train acc: 0.843750\n",
      "epoch 17: 36/125 | train loss: 0.260694 | train acc: 0.875000\n",
      "epoch 17: 37/125 | train loss: 0.165797 | train acc: 0.968750\n",
      "epoch 17: 38/125 | train loss: 0.226312 | train acc: 0.937500\n",
      "epoch 17: 39/125 | train loss: 0.302032 | train acc: 0.906250\n",
      "epoch 17: 40/125 | train loss: 0.145276 | train acc: 0.968750\n",
      "epoch 17: 40/125 | test loss: 0.007469 | test acc: 0.870044\n",
      "epoch 17: 41/125 | train loss: 0.042041 | train acc: 1.000000\n",
      "epoch 17: 42/125 | train loss: 0.178002 | train acc: 0.937500\n",
      "epoch 17: 43/125 | train loss: 0.174035 | train acc: 0.937500\n",
      "epoch 17: 44/125 | train loss: 0.351808 | train acc: 0.875000\n",
      "epoch 17: 45/125 | train loss: 0.176564 | train acc: 0.937500\n",
      "epoch 17: 46/125 | train loss: 0.176511 | train acc: 0.937500\n",
      "epoch 17: 47/125 | train loss: 0.195204 | train acc: 0.906250\n",
      "epoch 17: 48/125 | train loss: 0.385332 | train acc: 0.843750\n",
      "epoch 17: 49/125 | train loss: 0.101434 | train acc: 0.968750\n",
      "epoch 17: 50/125 | train loss: 0.379325 | train acc: 0.781250\n",
      "epoch 17: 50/125 | test loss: 0.009082 | test acc: 0.873348\n",
      "epoch 17: 51/125 | train loss: 0.132852 | train acc: 0.968750\n",
      "epoch 17: 52/125 | train loss: 0.157318 | train acc: 0.906250\n",
      "epoch 17: 53/125 | train loss: 0.145293 | train acc: 0.906250\n",
      "epoch 17: 54/125 | train loss: 0.148619 | train acc: 0.937500\n",
      "epoch 17: 55/125 | train loss: 0.148344 | train acc: 0.937500\n",
      "epoch 17: 56/125 | train loss: 0.359442 | train acc: 0.906250\n",
      "epoch 17: 57/125 | train loss: 0.236534 | train acc: 0.968750\n",
      "epoch 17: 58/125 | train loss: 0.194463 | train acc: 0.937500\n",
      "epoch 17: 59/125 | train loss: 0.346949 | train acc: 0.937500\n",
      "epoch 17: 60/125 | train loss: 0.334322 | train acc: 0.875000\n",
      "epoch 17: 60/125 | test loss: 0.011292 | test acc: 0.877753\n",
      "epoch 17: 61/125 | train loss: 0.192754 | train acc: 0.906250\n",
      "epoch 17: 62/125 | train loss: 0.079836 | train acc: 1.000000\n",
      "epoch 17: 63/125 | train loss: 0.244423 | train acc: 0.906250\n",
      "epoch 17: 64/125 | train loss: 0.141395 | train acc: 0.968750\n",
      "epoch 17: 65/125 | train loss: 0.259733 | train acc: 0.906250\n",
      "epoch 17: 66/125 | train loss: 0.287929 | train acc: 0.937500\n",
      "epoch 17: 67/125 | train loss: 0.217178 | train acc: 0.937500\n",
      "epoch 17: 68/125 | train loss: 0.112069 | train acc: 0.968750\n",
      "epoch 17: 69/125 | train loss: 0.211958 | train acc: 0.937500\n",
      "epoch 17: 70/125 | train loss: 0.230293 | train acc: 0.968750\n",
      "epoch 17: 70/125 | test loss: 0.006671 | test acc: 0.867841\n",
      "epoch 17: 71/125 | train loss: 0.162656 | train acc: 0.937500\n",
      "epoch 17: 72/125 | train loss: 0.291616 | train acc: 0.906250\n",
      "epoch 17: 73/125 | train loss: 0.222933 | train acc: 0.906250\n",
      "epoch 17: 74/125 | train loss: 0.150200 | train acc: 1.000000\n",
      "epoch 17: 75/125 | train loss: 0.171277 | train acc: 0.937500\n",
      "epoch 17: 76/125 | train loss: 0.120953 | train acc: 0.968750\n",
      "epoch 17: 77/125 | train loss: 0.352882 | train acc: 0.843750\n",
      "epoch 17: 78/125 | train loss: 0.081712 | train acc: 0.968750\n",
      "epoch 17: 79/125 | train loss: 0.114593 | train acc: 0.968750\n",
      "epoch 17: 80/125 | train loss: 0.066042 | train acc: 1.000000\n",
      "epoch 17: 80/125 | test loss: 0.008842 | test acc: 0.868943\n",
      "epoch 17: 81/125 | train loss: 0.335379 | train acc: 0.906250\n",
      "epoch 17: 82/125 | train loss: 0.226446 | train acc: 0.906250\n",
      "epoch 17: 83/125 | train loss: 0.322758 | train acc: 0.843750\n",
      "epoch 17: 84/125 | train loss: 0.114011 | train acc: 0.968750\n",
      "epoch 17: 85/125 | train loss: 0.187816 | train acc: 0.937500\n",
      "epoch 17: 86/125 | train loss: 0.288489 | train acc: 0.906250\n",
      "epoch 17: 87/125 | train loss: 0.148856 | train acc: 0.968750\n",
      "epoch 17: 88/125 | train loss: 0.271072 | train acc: 0.906250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 17: 89/125 | train loss: 0.311685 | train acc: 0.968750\n",
      "epoch 17: 90/125 | train loss: 0.107478 | train acc: 0.937500\n",
      "epoch 17: 90/125 | test loss: 0.005632 | test acc: 0.870044\n",
      "epoch 17: 91/125 | train loss: 0.144829 | train acc: 0.937500\n",
      "epoch 17: 92/125 | train loss: 0.369739 | train acc: 0.906250\n",
      "epoch 17: 93/125 | train loss: 0.287053 | train acc: 0.906250\n",
      "epoch 17: 94/125 | train loss: 0.261411 | train acc: 0.906250\n",
      "epoch 17: 95/125 | train loss: 0.312565 | train acc: 0.812500\n",
      "epoch 17: 96/125 | train loss: 0.425787 | train acc: 0.906250\n",
      "epoch 17: 97/125 | train loss: 0.275452 | train acc: 0.906250\n",
      "epoch 17: 98/125 | train loss: 0.136463 | train acc: 0.937500\n",
      "epoch 17: 99/125 | train loss: 0.099547 | train acc: 1.000000\n",
      "epoch 17: 100/125 | train loss: 0.099321 | train acc: 1.000000\n",
      "epoch 17: 100/125 | test loss: 0.010460 | test acc: 0.870044\n",
      "epoch 17: 101/125 | train loss: 0.277866 | train acc: 0.875000\n",
      "epoch 17: 102/125 | train loss: 0.290452 | train acc: 0.906250\n",
      "epoch 17: 103/125 | train loss: 0.107613 | train acc: 0.968750\n",
      "epoch 17: 104/125 | train loss: 0.331544 | train acc: 0.812500\n",
      "epoch 17: 105/125 | train loss: 0.112995 | train acc: 0.968750\n",
      "epoch 17: 106/125 | train loss: 0.241841 | train acc: 0.906250\n",
      "epoch 17: 107/125 | train loss: 0.347289 | train acc: 0.906250\n",
      "epoch 17: 108/125 | train loss: 0.157236 | train acc: 0.968750\n",
      "epoch 17: 109/125 | train loss: 0.187403 | train acc: 0.937500\n",
      "epoch 17: 110/125 | train loss: 0.243858 | train acc: 0.875000\n",
      "epoch 17: 110/125 | test loss: 0.018432 | test acc: 0.867841\n",
      "epoch 17: 111/125 | train loss: 0.174332 | train acc: 0.937500\n",
      "epoch 17: 112/125 | train loss: 0.241710 | train acc: 0.937500\n",
      "epoch 17: 113/125 | train loss: 0.393663 | train acc: 0.906250\n",
      "epoch 17: 114/125 | train loss: 0.091678 | train acc: 0.968750\n",
      "epoch 17: 115/125 | train loss: 0.412326 | train acc: 0.843750\n",
      "epoch 17: 116/125 | train loss: 0.371703 | train acc: 0.875000\n",
      "epoch 17: 117/125 | train loss: 0.369511 | train acc: 0.843750\n",
      "epoch 17: 118/125 | train loss: 0.093550 | train acc: 1.000000\n",
      "epoch 17: 119/125 | train loss: 0.354198 | train acc: 0.843750\n",
      "epoch 17: 120/125 | train loss: 0.568683 | train acc: 0.812500\n",
      "epoch 17: 120/125 | test loss: 0.009811 | test acc: 0.876652\n",
      "epoch 17: 121/125 | train loss: 0.238260 | train acc: 0.937500\n",
      "epoch 17: 122/125 | train loss: 0.093486 | train acc: 0.968750\n",
      "epoch 17: 123/125 | train loss: 0.431567 | train acc: 0.750000\n",
      "epoch 17: 124/125 | train loss: 0.154938 | train acc: 0.968750\n",
      "epoch 17: 125/125 | train loss: 0.245415 | train acc: 0.625000\n",
      "epoch 17 | mean train acc: 0.918500\n",
      "epoch 17 | mean test acc: 0.870503\n",
      "epoch 18: 1/125 | train loss: 0.322997 | train acc: 0.906250\n",
      "epoch 18: 2/125 | train loss: 0.099873 | train acc: 1.000000\n",
      "epoch 18: 3/125 | train loss: 0.318870 | train acc: 0.875000\n",
      "epoch 18: 4/125 | train loss: 0.081901 | train acc: 1.000000\n",
      "epoch 18: 5/125 | train loss: 0.255420 | train acc: 0.875000\n",
      "epoch 18: 6/125 | train loss: 0.211820 | train acc: 0.906250\n",
      "epoch 18: 7/125 | train loss: 0.194188 | train acc: 0.937500\n",
      "epoch 18: 8/125 | train loss: 0.183526 | train acc: 0.968750\n",
      "epoch 18: 9/125 | train loss: 0.370187 | train acc: 0.875000\n",
      "epoch 18: 10/125 | train loss: 0.153265 | train acc: 0.906250\n",
      "epoch 18: 10/125 | test loss: 0.015501 | test acc: 0.867841\n",
      "epoch 18: 11/125 | train loss: 0.108141 | train acc: 0.968750\n",
      "epoch 18: 12/125 | train loss: 0.367623 | train acc: 0.875000\n",
      "epoch 18: 13/125 | train loss: 0.250646 | train acc: 0.875000\n",
      "epoch 18: 14/125 | train loss: 0.144347 | train acc: 0.906250\n",
      "epoch 18: 15/125 | train loss: 0.109929 | train acc: 0.937500\n",
      "epoch 18: 16/125 | train loss: 0.175683 | train acc: 0.968750\n",
      "epoch 18: 17/125 | train loss: 0.099378 | train acc: 0.968750\n",
      "epoch 18: 18/125 | train loss: 0.312785 | train acc: 0.906250\n",
      "epoch 18: 19/125 | train loss: 0.238966 | train acc: 0.937500\n",
      "epoch 18: 20/125 | train loss: 0.221887 | train acc: 0.937500\n",
      "epoch 18: 20/125 | test loss: 0.013381 | test acc: 0.868943\n",
      "epoch 18: 21/125 | train loss: 0.221942 | train acc: 0.937500\n",
      "epoch 18: 22/125 | train loss: 0.228529 | train acc: 0.906250\n",
      "epoch 18: 23/125 | train loss: 0.278539 | train acc: 0.875000\n",
      "epoch 18: 24/125 | train loss: 0.382570 | train acc: 0.875000\n",
      "epoch 18: 25/125 | train loss: 0.174252 | train acc: 0.906250\n",
      "epoch 18: 26/125 | train loss: 0.258655 | train acc: 0.906250\n",
      "epoch 18: 27/125 | train loss: 0.155126 | train acc: 0.968750\n",
      "epoch 18: 28/125 | train loss: 0.340202 | train acc: 0.937500\n",
      "epoch 18: 29/125 | train loss: 0.099961 | train acc: 1.000000\n",
      "epoch 18: 30/125 | train loss: 0.195647 | train acc: 0.937500\n",
      "epoch 18: 30/125 | test loss: 0.006583 | test acc: 0.868943\n",
      "epoch 18: 31/125 | train loss: 0.299444 | train acc: 0.937500\n",
      "epoch 18: 32/125 | train loss: 0.251236 | train acc: 0.906250\n",
      "epoch 18: 33/125 | train loss: 0.109462 | train acc: 1.000000\n",
      "epoch 18: 34/125 | train loss: 0.219154 | train acc: 0.906250\n",
      "epoch 18: 35/125 | train loss: 0.230147 | train acc: 0.875000\n",
      "epoch 18: 36/125 | train loss: 0.290417 | train acc: 0.875000\n",
      "epoch 18: 37/125 | train loss: 0.134479 | train acc: 0.968750\n",
      "epoch 18: 38/125 | train loss: 0.219963 | train acc: 0.906250\n",
      "epoch 18: 39/125 | train loss: 0.155091 | train acc: 0.968750\n",
      "epoch 18: 40/125 | train loss: 0.343387 | train acc: 0.812500\n",
      "epoch 18: 40/125 | test loss: 0.009066 | test acc: 0.871145\n",
      "epoch 18: 41/125 | train loss: 0.058365 | train acc: 1.000000\n",
      "epoch 18: 42/125 | train loss: 0.227641 | train acc: 0.906250\n",
      "epoch 18: 43/125 | train loss: 0.267608 | train acc: 0.906250\n",
      "epoch 18: 44/125 | train loss: 0.364116 | train acc: 0.843750\n",
      "epoch 18: 45/125 | train loss: 0.124655 | train acc: 0.968750\n",
      "epoch 18: 46/125 | train loss: 0.298353 | train acc: 0.875000\n",
      "epoch 18: 47/125 | train loss: 0.245352 | train acc: 0.906250\n",
      "epoch 18: 48/125 | train loss: 0.054562 | train acc: 0.968750\n",
      "epoch 18: 49/125 | train loss: 0.180492 | train acc: 0.937500\n",
      "epoch 18: 50/125 | train loss: 0.314261 | train acc: 0.812500\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-4a678e9f75b0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     37\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m10\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;31m#             test_acc_epoch.append(test_model_simple(classifier, test_loader, step))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m             \u001b[0mtest_acc_epoch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_model_full\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum2cat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_epoch_cumulatiove_base\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m         \u001b[1;31m# to next batch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-faca1f57266b>\u001b[0m in \u001b[0;36mtest_model_full\u001b[1;34m(classifier, test_data, num2cat, step, model_epoch_cumulatiove_base)\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[0mall_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[0mall_choice\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m         \u001b[0mpoints\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[0mpoints\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpoints\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\dmitriy\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    344\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    345\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 346\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    347\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    348\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\dmitriy\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\dmitriy\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-e4349af44859>\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpg_dataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpg_dataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"train size: \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m//\u001b[0m\u001b[0mbatchsize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\dmitriy\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch_geometric\\data\\in_memory_dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     70\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mslice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\dmitriy\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch_geometric\\transforms\\sample_points.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[0marea\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mpos\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mface\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mpos\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mface\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcross\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpos\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mface\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mpos\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mface\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m         \u001b[0marea\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marea\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[0mprob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marea\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0marea\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\dmitriy\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mnorm\u001b[1;34m(self, p, dim, keepdim, dtype)\u001b[0m\n\u001b[0;32m    305\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mnorm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"fro\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    306\u001b[0m         \u001b[1;34mr\"\"\"See :func:`torch.norm`\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 307\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    309\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mlu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpivot\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mget_infos\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\dmitriy\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\functional.py\u001b[0m in \u001b[0;36mnorm\u001b[1;34m(input, p, dim, keepdim, out, dtype)\u001b[0m\n\u001b[0;32m    695\u001b[0m             \u001b[0mdim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mndim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    696\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 697\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_VariableFunctions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkeepdim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    698\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    699\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_VariableFunctions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkeepdim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "    train_acc_epoch, test_acc_epoch = [], []\n",
    "    for i, data in enumerate(train_loader):\n",
    "        points, labels = data\n",
    "        points = points.transpose(2, 1)\n",
    "        points, labels = points.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        classifier = classifier.train()\n",
    "        pred, _ = classifier(points)\n",
    "        pred = pred.view(-1, num_classes)\n",
    "        loss = F.nll_loss(pred, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        pred_choice = pred.data.max(1)[1]\n",
    "        correct = pred_choice.eq(labels.data).cpu().sum()\n",
    "        train_acc = correct.item() / float(batchsize)\n",
    "        print('epoch %d: %d/%d | train loss: %f | train acc: %f' % (model_epoch_cumulatiove_base+epoch+1, i+1, num_batch+1, loss.item(), train_acc))\n",
    "        train_acc_epoch.append(train_acc)\n",
    "        \n",
    "        # log train\n",
    "        encountered_class_nums = np.unique( np.array(labels.cpu().tolist()))\n",
    "        prec, recall, f1, _ = precision_recall_fscore_support(labels.cpu().tolist(), pred_choice.cpu().tolist(), labels=sorted(encountered_class_nums))\n",
    "        for j, num in enumerate(encountered_class_nums):\n",
    "            name = num2cat[num]\n",
    "            mlflow.log_metrics({\n",
    "                f\"{name}_acc\": prec[j],\n",
    "                f\"{name}_recall\": recall[j],\n",
    "                f\"{name}_f1\": f1[j],\n",
    "            }, step=step)\n",
    "        cnf_mtrx = confusion_matrix(labels.cpu().tolist(), pred_choice.cpu().tolist(), labels=sorted(list(num2cat)))\n",
    "        conf_mtrx_file_path = os.path.join(\"temp\", f\"train_cnf_mtrx_{epoch}_{i}.png\")\n",
    "        plot_conf_matrix(cnf_mtrx, [num2cat[num] for num in sorted(list(num2cat))], conf_mtrx_file_path)\n",
    "        mlflow.log_artifact(conf_mtrx_file_path)\n",
    "        mlflow.log_metric('train_acc', train_acc, step=step)\n",
    "\n",
    "        if (i+1) % 10 == 0:\n",
    "#             test_acc_epoch.append(test_model_simple(classifier, test_loader, step))\n",
    "            test_acc_epoch.append(test_model_full(classifier, test_dataset, num2cat, step, model_epoch_cumulatiove_base))\n",
    "            \n",
    "        # to next batch\n",
    "        step += 1\n",
    "    \n",
    "    # to next epoch\n",
    "    print(yellow('epoch %d | mean train acc: %f') % (model_epoch_cumulatiove_base+epoch+1, np.mean(train_acc_epoch)))\n",
    "    print(red('epoch %d | mean test acc: %f') % (model_epoch_cumulatiove_base+epoch+1, np.mean(test_acc_epoch)))\n",
    "    torch.save(classifier.state_dict(), '%s/%s_model_%d.pth' % ('models', 'model_10', epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "num2class = dict(zip(range(10), train_loader.dataset.raw_file_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bed\n",
      "-0.8752633 0.89645064 1.771714\n",
      "-0.99999905 0.89762324 1.8976223\n",
      "-0.231328 0.4611699 0.6924979\n",
      "\n",
      "desk\n",
      "-0.25317165 0.8512118 1.1043835\n",
      "-0.95747113 0.999999 1.9574702\n",
      "-0.6959119 0.12586308 0.82177496\n",
      "\n",
      "sofa\n",
      "-0.71917844 0.6585192 1.3776977\n",
      "-0.8147365 0.99999905 1.8147355\n",
      "-0.1762224 0.4483075 0.6245299\n",
      "\n",
      "chair\n",
      "-0.6121519 0.58003956 1.1921915\n",
      "-0.8185277 0.99923277 1.8177605\n",
      "-0.56900716 0.8437649 1.412772\n",
      "\n",
      "table\n",
      "-0.2510807 0.5558331 0.8069138\n",
      "-0.36220503 0.36191383 0.7241188\n",
      "-0.999999 0.10629373 1.1062927\n",
      "\n",
      "night_stand\n",
      "-0.10616867 0.7600212 0.8661899\n",
      "-0.79955983 0.79955995 1.5991198\n",
      "-0.999999 0.9989007 1.9988997\n",
      "\n",
      "monitor\n",
      "-0.3327983 0.999999 1.3327973\n",
      "-0.66639864 0.66639864 1.3327973\n",
      "-0.5120062 0.8207911 1.3327973\n",
      "\n",
      "monitor\n",
      "-0.1636657 0.39862558 0.56229126\n",
      "-0.6964982 0.67877895 1.3752772\n",
      "-0.28227845 0.999999 1.2822775\n",
      "\n",
      "sofa\n",
      "-0.92549175 0.36221954 1.2877113\n",
      "-0.46057156 0.9548646 1.4154361\n",
      "-0.2584274 0.10073888 0.3591663\n",
      "\n",
      "sofa\n",
      "-0.5206773 0.2951557 0.81583303\n",
      "-0.99939686 0.99364084 1.9930377\n",
      "-0.48419538 0.30367824 0.7878736\n",
      "\n",
      "toilet\n",
      "-0.4552204 0.50236785 0.95758826\n",
      "-0.76645243 0.6034657 1.3699181\n",
      "-0.71473897 0.95712274 1.6718616\n",
      "\n",
      "dresser\n",
      "-0.114071645 0.62443316 0.7385048\n",
      "-0.61424726 0.6165246 1.2307718\n",
      "-0.8530104 0.999999 1.8530095\n",
      "\n",
      "table\n",
      "-0.6117345 0.63331324 1.2450478\n",
      "-0.99751294 0.99999905 1.997512\n",
      "-0.3702162 0.15196715 0.52218336\n",
      "\n",
      "monitor\n",
      "-0.26349068 0.67495644 0.9384471\n",
      "-0.72137904 0.99999905 1.7213781\n",
      "-0.40282714 0.82389855 1.2267257\n",
      "\n",
      "sofa\n",
      "-0.29866958 0.41310963 0.71177924\n",
      "-0.99999905 0.99783474 1.9978337\n",
      "-0.24019453 0.32201758 0.5622121\n",
      "\n",
      "bathtub\n",
      "-0.8789597 0.7400989 1.6190586\n",
      "-0.99999905 0.61905956 1.6190586\n",
      "-0.16641697 0.33953804 0.505955\n",
      "\n",
      "table\n",
      "-0.47320879 0.48140267 0.9546114\n",
      "-0.99999905 0.97148114 1.9714801\n",
      "-0.43225828 0.22144301 0.6537013\n",
      "\n",
      "chair\n",
      "-0.38480023 0.39509654 0.77989674\n",
      "-0.3967622 0.5232639 0.92002606\n",
      "-0.7747185 0.999999 1.7747176\n",
      "\n",
      "monitor\n",
      "-0.015631767 0.286343 0.30197477\n",
      "-0.53088284 0.8591431 1.3900259\n",
      "-0.24118198 0.999999 1.241181\n",
      "\n",
      "toilet\n",
      "-0.4235513 0.5099513 0.93350255\n",
      "-0.9911959 0.5632827 1.5544786\n",
      "-0.8378616 0.76256543 1.600427\n",
      "\n",
      "monitor\n",
      "-0.08780523 0.3969442 0.48474944\n",
      "-0.94189286 0.9346774 1.8765702\n",
      "-0.62281144 0.9999989 1.6228104\n",
      "\n",
      "monitor\n",
      "-0.3458625 0.6002558 0.9461183\n",
      "-0.6383226 0.9166274 1.55495\n",
      "-0.45766035 0.9999191 1.4575795\n",
      "\n",
      "bed\n",
      "-0.4807979 0.48151955 0.96231747\n",
      "-0.999999 0.32732812 1.3273271\n",
      "-0.29757348 0.22321852 0.520792\n",
      "\n",
      "sofa\n",
      "-0.9999989 0.48033732 1.4803362\n",
      "-0.8890247 0.82903904 1.7180637\n",
      "-0.17606185 0.15890658 0.33496845\n",
      "\n",
      "bed\n",
      "-0.26995608 0.46525472 0.7352108\n",
      "-0.999999 0.5865086 1.5865076\n",
      "-0.3628139 0.3288647 0.6916786\n",
      "\n",
      "chair\n",
      "-0.43191916 0.37656483 0.80848396\n",
      "-0.44643652 0.37111592 0.81755245\n",
      "-0.4653814 0.9946074 1.4599888\n",
      "\n",
      "sofa\n",
      "-0.5015735 0.4716685 0.97324204\n",
      "-0.9990344 0.9894393 1.9884737\n",
      "-0.30356026 0.3957905 0.6993507\n",
      "\n",
      "toilet\n",
      "-0.61118966 0.8925782 1.5037678\n",
      "-0.9995026 0.7635262 1.7630289\n",
      "-0.75148773 0.7567594 1.5082471\n",
      "\n",
      "dresser\n",
      "-0.19156232 0.92825 1.1198124\n",
      "-0.9999989 0.86635476 1.8663538\n",
      "-0.67663735 0.5675985 1.2442359\n",
      "\n",
      "dresser\n",
      "-0.20152065 0.61138785 0.81290853\n",
      "-0.61225384 0.61225384 1.2245077\n",
      "-0.99999905 0.5640781 1.5640771\n",
      "\n",
      "bed\n",
      "-0.6828997 0.6906824 1.3735821\n",
      "-0.994555 0.99948746 1.9940424\n",
      "-0.43202317 0.5505934 0.98261654\n",
      "\n",
      "sofa\n",
      "-0.31156394 0.40073374 0.7122977\n",
      "-0.99947596 0.999999 1.999475\n",
      "-0.2959003 0.25023946 0.5461397\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pointss = data.pos.numpy()\n",
    "for btch in np.unique(data.batch.numpy()):\n",
    "    points = pointss[data.batch == btch]\n",
    "    print(num2class[data.y[btch].numpy().item()])\n",
    "    print(np.min(points[:, 0], axis=0), np.max(points[:, 0], axis=0), np.max(points[:, 0], axis=0) - np.min(points[:, 0], axis=0))\n",
    "    print(np.min(points[:, 1], axis=0), np.max(points[:, 1], axis=0), np.max(points[:, 1], axis=0) - np.min(points[:, 1], axis=0))\n",
    "    print(np.min(points[:, 2], axis=0), np.max(points[:, 2], axis=0), np.max(points[:, 2], axis=0) - np.min(points[:, 2], axis=0))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 7 1 7 7 8 1 7 2 1 6 7 2 4 9 2 5 9 6 8 7 2 8 9 1 5 9 3 2 7 5 7]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pptk.viewer.viewer.viewer at 0x15ea8f32588>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for data in train_loader:\n",
    "    pass\n",
    "    break\n",
    "    \n",
    "print(data.y.numpy())\n",
    "points = data.pos.numpy()[data.batch == 0]\n",
    "view(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
